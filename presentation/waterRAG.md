---
layout: persian  # ÛŒØ§ single Ø¨Ø§ Ú©Ù„Ø§Ø³ rtl-layout
classes: wide rtl-layout
dir: rtl
title: "ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ Ø¨Ø§ Ø¨Ù‡Ø±Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¢Ù…Ø§Ø± Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø´Ø±Ú©Øª Ø¢Ø¨ Ù…Ù†Ø·Ù‚Ù‡ Ø§ÛŒ Ø®Ø±Ø§Ø³Ø§Ù† Ø±Ø¶ÙˆÛŒ"
permalink: /presentation/waterRAG/
author_profile: true
sidebar:
  nav: "presentaton"
header:
  overlay_image: "/assets/images/background.jpg"
  overlay_filter: 0.3
  overlay_color: "#5e616c"
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
---

# Ø³Ø§Ù…Ø§Ù†Ù‡ RAG Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø³Ù†Ø§Ø¯ Ø¢Ø¨ÛŒ
## Document Management System with Retrieval-Augmented Generation for Water Projects

---

## ÙÙ‡Ø±Ø³Øª Ù…Ø­ØªÙˆÛŒØ§Øª

1. [Ù…Ù‚Ø¯Ù…Ù‡ Ùˆ Ù‡Ø¯Ù](#Ù…Ù‚Ø¯Ù…Ù‡-Ùˆ-Ù‡Ø¯Ù)
2. [Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ…](#Ù…Ø¹Ù…Ø§Ø±ÛŒ-Ø³ÛŒØ³ØªÙ…)
3. [Ø§Ø¬Ø²Ø§ÛŒ Ø§ØµÙ„ÛŒ](#Ø§Ø¬Ø²Ø§ÛŒ-Ø§ØµÙ„ÛŒ)
4. [ØªÙØµÛŒÙ„ Ú©Ø§Ù…Ù„ Ú©Ø¯Ù‡Ø§](#ØªÙØµÛŒÙ„-Ú©Ø§Ù…Ù„-Ú©Ø¯Ù‡Ø§)
5. [ÙØ±Ø¢ÛŒÙ†Ø¯ Ø´Ø§Ø®Øµâ€ŒØ³Ø§Ø²ÛŒ](#ÙØ±Ø¢ÛŒÙ†Ø¯-Ø´Ø§Ø®Øµâ€ŒØ³Ø§Ø²ÛŒ)
6. [Ø³ÛŒØ³ØªÙ… Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯](#Ø³ÛŒØ³ØªÙ…-Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ-Ù‡ÙˆØ´Ù…Ù†Ø¯)
7. [ÙˆØ§Ø³Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ùˆ API](#ÙˆØ§Ø³Ø·-Ú©Ø§Ø±Ø¨Ø±ÛŒ-Ùˆ-api)
8. [Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡](#Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ-Ù†Ù…ÙˆÙ†Ù‡)

---

## Ù…Ù‚Ø¯Ù…Ù‡ Ùˆ Ù‡Ø¯Ù

### Ø§Ù‡Ø¯Ø§Ù Ø³Ø§Ù…Ø§Ù†Ù‡ RAG

Ø§ÛŒÙ† Ø³Ø§Ù…Ø§Ù†Ù‡ Ø¨Ø§ Ù‡Ø¯Ù ÙØ±Ø§Ù‡Ù…â€ŒÚ©Ø±Ø¯Ù† ÛŒÚ© Ø±Ø§Ø¨Ø· Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¬Ù‡Øª Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø§Ø³Ù†Ø§Ø¯ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø¨ÛŒ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ù‡ Ú©Ù…Ú© ÙÙ†Ø§ÙˆØ±ÛŒ **Retrieval-Augmented Generation (RAG)**ØŒ Ø³ÛŒØ³ØªÙ… Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯:

- **Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ:** Ø¯Ø±Ú© Ù…Ø¹Ù†ÛŒ Ùˆ Ù…ÙÙ‡ÙˆÙ… Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ø§Ø±Ø¨Ø±ØŒ Ù†Ù‡ ØµØ±Ù Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
- **Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡:** ÛŒØ§ÙØªÙ† Ø§Ø³Ù†Ø§Ø¯ Ùˆ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø² Ù…ÛŒØ§Ù† Ù‡Ø²Ø§Ø±Ø§Ù† ØµÙØ­Ù‡
- **Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯:** ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ´Ù† Ùˆ Ù…Ù†Ø·Ù‚ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³Ù†Ø§Ø¯
- **ØªÙˆØ¬ÛŒÙ‡ ØªØµÙ…ÛŒÙ…:** Ø§Ø±Ø§Ø¦Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ù…Ø±Ø§Ø¬Ø¹ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù¾Ø§Ø³Ø®

### Ú†Ø±Ø§ÛŒÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² RAG

Ø¨Ø¯ÙˆÙ† RAG:
```
Ú©Ø§Ø±Ø¨Ø±: "Ú†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ ØªØµÙÛŒÙ‡â€ŒØ®Ø§Ù†Ù‡ Ø±Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø±Ø¯ØŸ"
LLM: [Ù¾Ø§Ø³Ø® Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ø·Ù„Ø§Ø¹ Ø§Ø² Ø§Ø³Ù†Ø§Ø¯ Ù…Ø­Ù„ÛŒ]
```

Ø¨Ø§ RAG:
```
Ú©Ø§Ø±Ø¨Ø±: "Ú†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ ØªØµÙÛŒÙ‡â€ŒØ®Ø§Ù†Ù‡ Ø±Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø±Ø¯ØŸ"
Ø³ÛŒØ³ØªÙ…: [Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù…Ø´Ù‡Ø¯ + Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù†ØªØ§ÛŒØ¬ Ù…Ø±ØªØ¨Ø·]
LLM: [ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³Ù†Ø§Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ]
Ú©Ø§Ø±Ø¨Ø±: [Ù¾Ø§Ø³Ø® + Ù…Ù†Ø§Ø¨Ø¹]
```

---

## Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ…

### Ù†Ù…Ø§ÛŒ Ú©Ù„ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ù„Ø§ÛŒÙ‡ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø®                         â”‚
â”‚              (Flask Web App + SocketIO)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø±Ø³Ø´    â”‚  â”‚  ØªÙˆÙ„ÙŠØ¯ Ù¾Ø§Ø³Ø®      â”‚
        â”‚  (Query Engine) â”‚  â”‚  (LLM Engine)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Ø³ÛŒØ³ØªÙ… Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ (RAG)      â”‚
        â”‚  - Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ (FAISS)          â”‚
        â”‚  - Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ (BM25)      â”‚
        â”‚  - ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ (RRF)              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø´Ø§Ø®Øµ    â”‚
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚ â”‚ Ù…ØªØ§Ø¯ÛŒØªØ§ (JSON)       â”‚  â”‚
    â”‚ â”‚ Ø´Ø§Ø®Øµ FAISS          â”‚  â”‚
    â”‚ â”‚ Ø´Ø§Ø®Øµ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ        â”‚  â”‚
    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Ø§Ø³Ù†Ø§Ø¯ Ù…Ù†Ø¨Ø¹             â”‚
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚ â”‚ WaterOptimizing.md   â”‚  â”‚
    â”‚ â”‚ water1.docx          â”‚  â”‚
    â”‚ â”‚ Ø§Ø³Ù†Ø§Ø¯ Ø¯ÛŒÚ¯Ø±           â”‚  â”‚
    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ…

```
1. Ø´Ø§Ø®Øµâ€ŒØ³Ø§Ø²ÛŒ (Indexing)
   â””â”€ Ø³Ù†Ø¯ â†’ Ú†Ø§Ù†Ú©ÛŒÙ†Ú¯ â†’ Embedding â†’ FAISS Index
   
2. Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ (Retrieval)
   â””â”€ Ù¾Ø±Ø³Ø´ â†’ Ù…Ø¹Ù…ÙˆÙ„â€ŒØ³Ø§Ø²ÛŒ â†’ Ø¬Ø³ØªØ¬Ùˆ Ù…Ø¹Ù†Ø§ÛŒÛŒ + Ú©Ù„ÛŒØ¯ÛŒ
   
3. ØªÙˆÙ„ÛŒØ¯ (Generation)
   â””â”€ Ù¾Ø±Ø³Ø´ + Ù…Ù†Ø§Ø¨Ø¹ â†’ LLM (Gemini) â†’ Ù¾Ø§Ø³Ø® ØªÙˆØ¶ÛŒØ­ÛŒ
```

---

## Ø§Ø¬Ø²Ø§ÛŒ Ø§ØµÙ„ÛŒ

### 1. Ù…Ø§Ú˜ÙˆÙ„ Ø´Ø§Ø®Øµâ€ŒØ³Ø§Ø²ÛŒ (`create_dms_index.py`)

**Ù‡Ø¯Ù:** ØªØ¨Ø¯ÛŒÙ„ Ø§Ø³Ù†Ø§Ø¯ Ø®Ø§Ù… Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡

#### ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:

- **OCR Cleanup:** Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø®Ø·Ø§Ù‡Ø§ÛŒ OCR
- **Smart Chunking:** ØªÙ‚Ø³ÛŒÙ… Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø§Ø³Ù†Ø§Ø¯
- **Embedding Generation:** ØªÙˆÙ„ÛŒØ¯ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
- **FAISS Indexing:** Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø§Ø®Øµ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹

#### ÙÙ„ÙˆÚ†Ø§Ø±Øª ØªÙØµÛŒÙ„ÛŒ:

```python
# 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³Ù†Ø¯
content, metadata = load_document(file_path)

# 2. Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ OCR
if clean_ocr:
    content = ocr_cleaner.clean(content)

# 3. ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
chunks = smart_chunker.chunk_document(content, metadata)

# 4. ØªÙˆÙ„ÛŒØ¯ Embedding
embeddings = embedding_model.encode(chunks)

# 5. Ø§ÛŒØ¬Ø§Ø¯ FAISS Index
faiss_index = create_faiss_index(embeddings)

# 6. Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§
save_metadata(chunks, embeddings)
```

### 2. Ù…Ø§Ú˜ÙˆÙ„ Smart Chunker (`src/smart_chunker.py`)

**Ù‡Ø¯Ù:** ØªÙ‚Ø³ÛŒÙ… Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø§Ø³Ù†Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ

#### Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ú†Ø§Ù†Ú©ÛŒÙ†Ú¯:

```python
class SmartChunker:
    """
    ØªÙ‚Ø³ÛŒÙ…â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù†Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ
    """
    
    # Ø§ÙˆÙ„ÙˆÛŒØª Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§
    separators = [
        "\n# ",          # Ø³Ø±ØªÛŒØªØ± Ø§ØµÙ„ÛŒ
        "\nÙ…Ø§Ú˜ÙˆÙ„ ",       # Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ú˜ÙˆÙ„
        "\nØ¬Ø¯ÙˆÙ„ ",       # Ø¬Ø¯Ø§ÙˆÙ„
        "\n## ",         # Ø²ÛŒØ±Ø³Ø±ØªÛŒØªØ±
        "\n\n",          # ØªÙ‚Ø³ÛŒÙ… Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
        "\n",            # ØªÙ‚Ø³ÛŒÙ… Ø®Ø·
        "Ø› ",            # Ù†Ù‚Ø·Ù‡â€ŒÙˆÛŒØ±Ú¯ÙˆÙ„ ÙØ§Ø±Ø³ÛŒ
        " "              # ÙØ§ØµÙ„Ù‡
    ]
```

#### Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ:

```
Ø³Ù†Ø¯ Ø§ØµÙ„ÛŒ:
===========================
# Ù…Ø§Ú˜ÙˆÙ„ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù†Ø§Ø¨Ø¹

## Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù†ÛŒØ±ÙˆÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ

Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´...
(300 Ú©Ù„Ù…Ù‡)

## Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ù…Ø§Ø´ÛŒÙ†â€ŒØ¢Ù„Ø§Øª

ØªØ¬Ù‡ÛŒØ²Ø§Øª Ø´Ø§Ù…Ù„...
(350 Ú©Ù„Ù…Ù‡)
===========================

Chunks Ù†Ù‡Ø§ÛŒÛŒ:
Chunk 1: "# Ù…Ø§Ú˜ÙˆÙ„ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù†Ø§Ø¨Ø¹\n## Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù†ÛŒØ±ÙˆÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ\n..." [~512 tokens]
Chunk 2: "## Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ù…Ø§Ø´ÛŒÙ†â€ŒØ¢Ù„Ø§Øª\n..." [~512 tokens]
```

### 3. Ù…Ø§Ú˜ÙˆÙ„ OCR Cleanup (`src/ocr_cleanup.py`)

**Ù‡Ø¯Ù:** Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª Ø§Ø³Ù†Ø§Ø¯ OCRâ€ŒØ´Ø¯Ù‡

#### Ø§Ù†ÙˆØ§Ø¹ Ø§ØµÙ„Ø§Ø­Ø§Øª:

```python
ocr_fixes = {
    # Ø®Ø·Ø§Ù‡Ø§ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±ÛŒ
    'character_level': {
        'Ùƒ' â†’ 'Ú©',  # Ú©Ø§Ù Ø¹Ø±Ø¨ÛŒ â†’ Ú©Ø§Ù ÙØ§Ø±Ø³ÛŒ
        'ÙŠ' â†’ 'ÛŒ',  # ÛŒØ§Ø¡ Ø¹Ø±Ø¨ÛŒ â†’ ÛŒØ§Ø¡ ÙØ§Ø±Ø³ÛŒ
        '  ' â†’ ' '   # ÙØ§ØµÙ„Ù‡ Ø¯ÙˆÚ¯Ø§Ù†Ù‡ â†’ ØªÚ©ÛŒ
    },
    
    # Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø³Ø·Ø­ Ú©Ù„Ù…Ù‡
    'word_level': {
        'ØµÙØ­Ø© 5Ø§Ø² 10' â†’ 'ØµÙØ­Ù‡ 5 Ø§Ø² 10'
        'Ø§Ø³Ú©Ø§Ø¯Ø§' â†’ 'SCADA'
    },
    
    # Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø§Ø¹Ø¯Ø§Ø¯
    'number_level': {
        'Ûµ8' â†’ '58'  # Ø§Ø¹Ø¯Ø§Ø¯ Ù…Ø®Ù„ÙˆØ· â†’ ÛŒÚ©Ø³Ø§Ù†
        '[0-9]+[Û°-Û¹]+' â†’ 'conversion'
    }
}
```

#### Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ú©ÛŒÙÛŒØª:

```python
quality_score = (
    (1 - character_errors/total_chars) * 0.3 +
    (1 - word_errors/total_words) * 0.3 +
    (1 - spacing_errors/issues) * 0.2 +
    (1 - encoding_errors/encoding_issues) * 0.2
)
# Ù†Ù…ÙˆÙ†Ù‡: 0.78 = 78% Ú©ÛŒÙÛŒØª
```

### 4. Ù…Ø§Ú˜ÙˆÙ„ Hybrid Retriever (`src/hybrid_retriever.py`)

**Ù‡Ø¯Ù:** Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ ØªØ±Ú©ÛŒØ¨ÛŒ

#### Ø¯Ùˆ Ø±ÙˆØ´ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ:

#### Ø§Ù„Ù) Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ (Semantic Search - FAISS)

```python
# ÙØ±Ø¢ÛŒÙ†Ø¯
query = "Ú†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù‡Ø´ Ø¯Ù‡ÛŒÙ…ØŸ"
query_embedding = embedding_model.encode(query)  # 1024-dim vector
distances, indices = faiss_index.search(query_embedding, k=5)
# Ù†ØªÛŒØ¬Ù‡: [index_1, index_5, index_12, ...] (Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ†â€ŒÙ‡Ø§)

# Ù…Ø²Ø§ÛŒØ§:
# - Ø¯Ø±Ú© Ù…Ø¹Ù†Ø§ Ùˆ Ù…ÙÙ‡ÙˆÙ…
# - Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø² Ù„Ø­Ø§Ø¸ Ù…ÙÙ‡ÙˆÙ…ÛŒ
# - Ú©Ø§Ø± Ú©Ø±Ø¯Ù† Ø¨Ø§ ØªØºÛŒÛŒØ±Ø§Øª Ú©Ù„Ù…Ø§Øª

# Ù†Ù…ÙˆÙ†Ù‡:
Query: "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø²ÛŒÙ†Ù‡"
Ø¨Ø¯ÙˆÙ† Ø¯Ø±Ú© Ù…Ø¹Ù†Ø§: Ù†ØªØ§ÛŒØ¬ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ" ÛŒØ§ "Ù‡Ø²ÛŒÙ†Ù‡"
Ø¨Ø§ Ø¯Ø±Ú© Ù…Ø¹Ù†Ø§: Ù†ØªØ§ÛŒØ¬ Ø´Ø§Ù…Ù„ "Ú©Ø§Ù‡Ø´ Ù‡Ø²ÛŒÙ†Ù‡"ØŒ "ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ"ØŒ "Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù"
```

#### Ø¨) Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ (Keyword Search - BM25)

```python
# Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… BM25
BM25_score(doc, query) = Î£[IDF(q) * (f(q,D) * (k1+1)) / (f(q,D) + k1*(1-b+b*|D|/avgdl))]

# Ø¬Ø§ÛŒÛŒ Ú©Ù‡:
# IDF(q) = log((N - n(q) + 0.5) / (n(q) + 0.5))
# f(q,D) = ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙØ¹Ø§Øª Ú©Ù„Ù…Ù‡ q Ø¯Ø± Ø³Ù†Ø¯ D
# |D| = Ø·ÙˆÙ„ Ø³Ù†Ø¯
# avgdl = Ø·ÙˆÙ„ Ù…ØªÙˆØ³Ø· Ø§Ø³Ù†Ø§Ø¯

# Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ:
Query: "Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù†Ø§Ø¨Ø¹ Ø§Ù†Ø³Ø§Ù†ÛŒ"
Documents: [
    "Ù…Ù†Ø§Ø¨Ø¹ Ø§Ù†Ø³Ø§Ù†ÛŒ Ùˆ Ù…Ø§Ù„ÛŒ...",      BM25=8.5  âœ“
    "Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ù…Ù†Ø§Ø¨Ø¹",        BM25=5.2
    "Ù…Ø§Ø´ÛŒÙ†â€ŒØ¢Ù„Ø§Øª Ùˆ ØªØ¬Ù‡ÛŒØ²Ø§Øª",        BM25=0.1
]
```

#### Ø¬) ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ (RRF - Reciprocal Rank Fusion)

```python
# ÙØ±Ù…ÙˆÙ„ RRF
RRF_score(d) = Î£[1 / (k + rank(d,S))]

# Ø¬Ø§ÛŒÛŒ Ú©Ù‡ k=60 Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø«Ø§Ø¨Øª Ø§Ø³Øª

# Ù…Ø«Ø§Ù„:
Semantic_search_ranks:   [doc_1, doc_5, doc_3, ...]
Keyword_search_ranks:    [doc_1, doc_3, doc_7, ...]

# RRF scores:
doc_1: 1/(60+1) + 1/(60+1) = 0.033  â† Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²
doc_3: 1/(60+3) + 1/(60+2) = 0.031
doc_5: 1/(60+2) = 0.016
doc_7: 1/(60+2) = 0.016
```

### 5. Ø³Ø§Ù…Ø§Ù†Ù‡ RAG Ø§ØµÙ„ÛŒ (`dms_rag_system.py`)

**Ù‡Ø¯Ù:** Ú©Ø§Ù…Ù„â€ŒÚ©Ø±Ø¯Ù† ÙØ±Ø¢ÛŒÙ†Ø¯ RAG

#### Ø§Ø¬Ø²Ø§ÛŒ Ø§ØµÙ„ÛŒ:

```python
class DMSRAGSystem:
    def __init__(self):
        # 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ NLP
        self.embedding_model = SentenceTransformer(...)
        self.llm_model = genai.GenerativeModel(...)
        
        # 2. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§
        self.faiss_index = faiss.read_index(...)
        self.metadata = load_json(...)
        
        # 3. Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯
        self.retriever = HybridRetriever(...)
    
    def process_query(self, query):
        # Ù…Ø±Ø­Ù„Ù‡ 1: Ø¯Ø±Ú© Ùˆ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾Ø±Ø³Ø´
        query_type = self.classify_query(query)
        
        # Ù…Ø±Ø­Ù„Ù‡ 2: Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø±ØªØ¨Ø·
        sources = self.retriever.search(query, top_k=5)
        
        # Ù…Ø±Ø­Ù„Ù‡ 3: Ø³Ø§Ø®Øª Context Ø¨Ø±Ø§ÛŒ LLM
        context = self.build_context(sources)
        
        # Ù…Ø±Ø­Ù„Ù‡ 4: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® ØªÙˆØ³Ø· LLM
        answer = self.generate_answer(query, context)
        
        # Ù…Ø±Ø­Ù„Ù‡ 5: Ø¨Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù…Ù†Ø§Ø¨Ø¹
        return RAGResponse(
            query=query,
            answer=answer,
            sources=sources,
            confidence=self.calculate_confidence(...)
        )
```

### 6. Ø¨Ø±Ù†Ø§Ù…Ù‡ Flask (`app.py`)

**Ù‡Ø¯Ù:** Ø§Ø±Ø§Ø¦Ù‡ ÙˆØ§Ø³Ø· ÙˆØ¨ Ùˆ API

#### Ù†Ù‚Ø§Ø· Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ Ø§ØµÙ„ÛŒ:

```python
# 1. Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø§ØµÙ„ÛŒ
@app.route('/')
def index():
    return render_template('index.html')

# 2. Ø¨Ø±Ø±Ø³ÛŒ Ø³Ù„Ø§Ù…Øª Ø³ÛŒØ³ØªÙ…
@app.route('/api/health', methods=['GET'])
def health_check():
    return {
        'status': 'healthy',
        'statistics': {
            'total_documents': 150,
            'total_vectors': 5000,
            'embedding_model': 'intfloat/multilingual-e5-large'
        }
    }

# 3. Ù†Ù‚Ø·Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ Ø§ØµÙ„ÛŒ Ú†Øª
@app.route('/api/chat', methods=['POST'])
def chat():
    user_query = request.json.get('message')
    response = dms_rag.process_query(user_query)
    return jsonify({
        'answer': response.answer,
        'sources': response.sources,
        'confidence': response.confidence,
        'query_type': response.query_type
    })

# 4. Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ø¨Ù„Ø§Ø¯Ø±Ù†Ú¯
@socketio.on('message')
def handle_message(data):
    response = dms_rag.process_query(data['message'])
    emit('response', {
        'answer': response.answer,
        'sources': response.sources
    })
```

---

## ØªÙØµÛŒÙ„ Ú©Ø§Ù…Ù„ Ú©Ø¯Ù‡Ø§

### 1. Ú©Ø¯ Ø´Ø§Ø®Øµâ€ŒØ³Ø§Ø²ÛŒ (`create_dms_index.py`)

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù¾Ø§ÛŒÙ¾â€ŒÙ„Ø§ÛŒÙ† Ø´Ø§Ø®Øµâ€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù†Ø§Ø¯ DMS
Ø´Ø§Ù…Ù„: Chunking Ù…Ø¹Ù†Ø§ÛŒÛŒØŒ Embedding Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒØŒ FAISS Index Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡
"""

class DMSIndexBuilder:
    def __init__(self, 
                 documents_dir="./water_document",
                 output_dir="./water_index",
                 embedding_model="intfloat/multilingual-e5-large",
                 chunk_size=512,
                 use_ivf_index=False):
        """
        Ù…Ù‚Ø¯Ø§Ø±â€ŒØ¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø³Ø§Ø²Ù†Ø¯Ù‡ Ø´Ø§Ø®Øµ
        """
        self.documents_dir = Path(documents_dir)
        self.output_dir = Path(output_dir)
        self.embedding_model_name = embedding_model
        self.chunk_size = chunk_size
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Embedding
        print(f"ðŸ“¦ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {self.embedding_model_name}")
        self.embedding_model = SentenceTransformer(
            self.embedding_model_name, 
            device="cpu"
        )
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        print(f"âœ… Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯ (Ø§Ø¨Ø¹Ø§Ø¯: {self.embedding_dim})")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú†Ø§Ù†Ú©Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯
        self.chunker = SmartChunker(chunk_size=chunk_size)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾Ø§Ú©â€ŒÚ©Ù†Ù†Ø¯Ù‡ OCR
        self.ocr_cleaner = OCRCleaner()
    
    def _load_document(self, file_path):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø³Ù†Ø¯"""
        print(f"ðŸ“„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ: {file_path.name}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ø³Ù†Ø¯
        metadata = {
            'source_file': str(file_path),
            'filename': file_path.name,
            'file_size': len(content),
            'indexed_at': datetime.now().isoformat(),
        }
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ OCR
        if self.clean_ocr:
            print(f"   ðŸ§¹ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø®Ø·Ø§Ù‡Ø§ÛŒ OCR...")
            cleaned_content, clean_report = self.ocr_cleaner.clean(content)
            
            if clean_report['total_fixes'] > 0:
                print(f"   âœ“ {clean_report['total_fixes']} Ø§ØµÙ„Ø§Ø­ Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯")
                content = cleaned_content
                metadata['ocr_cleaned'] = True
        
        print(f"   âœ“ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ: {len(content):,} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        return content, metadata
    
    def _chunk_document(self, content, metadata):
        """ØªÙ‚Ø³ÛŒÙ… Ø³Ù†Ø¯ Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ"""
        return self.chunker.chunk_document(content, metadata)
    
    def _generate_embeddings_batch(self, texts, batch_size=32):
        """ØªÙˆÙ„ÛŒØ¯ Embeddingâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ"""
        print(f"ðŸ”¢ ØªÙˆÙ„ÛŒØ¯ Embedding Ø¨Ø±Ø§ÛŒ {len(texts)} Ù‚Ø·Ø¹Ù‡...")
        
        all_embeddings = []
        for i in tqdm(range(0, len(texts), batch_size)):
            batch = texts[i:i + batch_size]
            embeddings = self.embedding_model.encode(
                batch,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            all_embeddings.append(embeddings)
        
        embeddings_array = np.vstack(all_embeddings).astype('float32')
        print(f"   âœ… ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: Ø´Ú©Ù„ {embeddings_array.shape}")
        return embeddings_array
    
    def _create_faiss_index(self, embeddings):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø§Ø®Øµ FAISS"""
        print(f"ðŸ—‚ï¸ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø§Ø®Øµ FAISS...")
        
        n_vectors, dim = embeddings.shape
        
        if n_vectors > 1000:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² IVF Ø¨Ø±Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¨Ø²Ø±Ú¯
            nlist = min(int(np.sqrt(n_vectors)), 100)
            quantizer = faiss.IndexFlatL2(dim)
            index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_L2)
            index.train(embeddings)
            index.add(embeddings)
            print(f"   âœ… Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: IVF Index ({n_vectors} Ø¨Ø±Ø¯Ø§Ø±ØŒ {nlist} Ø®ÙˆØ´Ù‡)")
        else:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Flat Ø¨Ø±Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©ÙˆÚ†Ú©â€ŒØªØ±
            index = faiss.IndexFlatL2(dim)
            index.add(embeddings)
            print(f"   âœ… Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: Flat Index ({n_vectors} Ø¨Ø±Ø¯Ø§Ø±)")
        
        return index
    
    def build_index(self, file_pattern="*.md"):
        """Ø³Ø§Ø®Øª Ø´Ø§Ø®Øµ Ú©Ø§Ù…Ù„"""
        print(f"\n{'='*70}")
        print(f"ðŸš€ Ø´Ø±ÙˆØ¹ Ø³Ø§Ø®Øª Ø´Ø§Ø®Øµ Water")
        print(f"{'='*70}\n")
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        files = list(self.documents_dir.glob(file_pattern))
        print(f"ðŸ“ Ù¾ÛŒØ¯Ø§ Ø´Ø¯ {len(files)} ÙØ§ÛŒÙ„\n")
        
        all_embeddings = []
        all_metadata = []
        chunk_texts = []
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø± Ø³Ù†Ø¯
        for idx, file_path in enumerate(files):
            print(f"\n[{idx+1}/{len(files)}] Ù¾Ø±Ø¯Ø§Ø²Ø´: {file_path.name}")
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
            content, metadata = self._load_document(file_path)
            
            # ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ
            chunks = self._chunk_document(content, metadata)
            print(f"   âœ“ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ: {len(chunks)} Ù‚Ø·Ø¹Ù‡")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ùˆ Embedding
            for chunk in chunks:
                chunk_texts.append(chunk.text)
                all_metadata.append({
                    'text': chunk.text,
                    'chunk_id': chunk.id,
                    'source_file': metadata['source_file'],
                    'chunk_index': chunk.chunk_index,
                    'metadata': chunk.metadata
                })
        
        print(f"\n{'='*70}")
        print(f"ðŸ“Š Ø®Ù„Ø§ØµÙ‡:")
        print(f"   Ú©Ù„ Ø³Ù†Ø¯: {len(files)}")
        print(f"   Ú©Ù„ Ù‚Ø·Ø¹Ø§Øª: {len(chunk_texts)}")
        
        # ØªÙˆÙ„ÛŒØ¯ Embeddingâ€ŒÙ‡Ø§
        embeddings = self._generate_embeddings_batch(chunk_texts)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø§Ø®Øµ FAISS
        faiss_index = self._create_faiss_index(embeddings)
        
        # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
        print(f"\nðŸ’¾ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§...")
        faiss.write_index(faiss_index, str(self.output_dir / "dms_faiss_index.index"))
        
        with open(self.output_dir / "dms_metadata.json", 'w', encoding='utf-8') as f:
            json.dump(all_metadata, f, ensure_ascii=False, indent=2)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø®Ù„Ø§ØµÙ‡
        summary = {
            'total_documents': len(files),
            'total_chunks': len(chunk_texts),
            'embedding_model': self.embedding_model_name,
            'embedding_dimension': self.embedding_dim,
            'chunk_size': self.chunk_size,
            'indexed_at': datetime.now().isoformat()
        }
        
        with open(self.output_dir / "dms_index_summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'='*70}")
        print(f"âœ… Ø´Ø§Ø®Øµ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª!")
        print(f"   - FAISS Index: {self.output_dir / 'dms_faiss_index.index'}")
        print(f"   - Metadata: {self.output_dir / 'dms_metadata.json'}")
        print(f"   - Summary: {self.output_dir / 'dms_index_summary.json'}")
        print(f"{'='*70}\n")
        
        return summary
```

### 2. Ú©Ø¯ Smart Chunker

```python
class SmartChunker:
    """ØªÙ‚Ø³ÛŒÙ…â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù†Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ"""
    
    def chunk_document(self, content, metadata):
        """ØªÙ‚Ø³ÛŒÙ… Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø³Ù†Ø¯"""
        chunks = []
        chunk_index = 0
        
        # ÙØ§ØµÙ„Ù‡ Ø¨Ù†Ø¯ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        sections = self._split_by_headers(content)
        
        for section_text in sections:
            # Ø§Ú¯Ø± Ø¨Ø®Ø´ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ø§Ø´Ø¯ØŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†
            if self._estimate_tokens(section_text) > self.chunk_size * 2:
                sub_chunks = self._split_by_paragraphs(section_text)
            else:
                sub_chunks = [section_text]
            
            for chunk_text in sub_chunks:
                if len(chunk_text.strip()) > self.min_chunk_size:
                    chunk = Chunk(
                        id=f"chunk_{len(chunks):05d}",
                        text=chunk_text,
                        chunk_index=chunk_index,
                        metadata={
                            **metadata,
                            'chunk_size': len(chunk_text),
                            'estimated_tokens': self._estimate_tokens(chunk_text),
                            'content_type': self._detect_section_type(chunk_text),
                            'keywords': self._find_technical_keywords(chunk_text)
                        }
                    )
                    chunks.append(chunk)
                    chunk_index += 1
        
        return chunks
```

### 3. Ú©Ø¯ Hybrid Retriever

```python
class HybridRetriever:
    """Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ (Ù…Ø¹Ù†Ø§ÛŒÛŒ + Ú©Ù„ÛŒØ¯ÛŒ)"""
    
    def search(self, query, top_k=5):
        """Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ú©ÛŒØ¨ÛŒ"""
        
        # 1. Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
        query_embedding = self.embedding_model.encode([query])[0]
        distances, indices = self.faiss_index.search(
            np.array([query_embedding], dtype='float32'),
            k=top_k*2  # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ Ø¨Ø¹Ø¯ÛŒ
        )
        
        semantic_results = [
            (int(idx), 1.0 - dist)  # ØªØ¨Ø¯ÛŒÙ„ ÙØ§ØµÙ„Ù‡ Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª
            for idx, dist in zip(indices[0], distances[0])
            if idx >= 0
        ]
        
        # 2. Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„ÛŒØ¯ÛŒ (BM25)
        keyword_results = self.bm25.search(query, top_k=top_k*2)
        
        # 3. ØªØ±Ú©ÛŒØ¨ Ø¨Ø§ RRF
        combined_scores = self._reciprocal_rank_fusion(
            semantic_results,
            keyword_results
        )
        
        # 4. Ø§Ù†ØªØ®Ø§Ø¨ top_k Ù†ØªÛŒØ¬Ù‡
        top_results = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        return [
            {
                'chunk_id': self.metadata[idx]['chunk_id'],
                'text': self.metadata[idx]['text'],
                'source': self.metadata[idx]['source_file'],
                'confidence': score
            }
            for idx, score in top_results
        ]
    
    def _reciprocal_rank_fusion(self, semantic_results, keyword_results):
        """ØªØ±Ú©ÛŒØ¨ Ø¨Ø§ RRF"""
        combined = {}
        k = 60  # Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø«Ø§Ø¨Øª RRF
        
        for rank, (idx, score) in enumerate(semantic_results):
            combined[idx] = combined.get(idx, 0) + 1/(k + rank + 1)
        
        for rank, (idx, score) in enumerate(keyword_results):
            combined[idx] = combined.get(idx, 0) + 1/(k + rank + 1)
        
        return combined
```

### 4. Ú©Ø¯ DMS RAG System

```python
class DMSRAGSystem:
    """Ø³Ø§Ù…Ø§Ù†Ù‡ RAG Ú©Ø§Ù…Ù„"""
    
    def process_query(self, query):
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø±Ø³Ø´ Ú©Ø§Ù…Ù„"""
        start_time = time.time()
        
        # Ù…Ø±Ø­Ù„Ù‡ 1: Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾Ø±Ø³Ø´
        query_type = self._classify_query(query)
        print(f"ðŸ“ Ù†ÙˆØ¹ Ù¾Ø±Ø³Ø´: {query_type}")
        
        # Ù…Ø±Ø­Ù„Ù‡ 2: Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
        retrieval_start = time.time()
        sources = self.retriever.search(query, top_k=self.max_sources)
        retrieval_time = time.time() - retrieval_start
        
        # Ù…Ø±Ø­Ù„Ù‡ 3: Ø³Ø§Ø®Øª Context
        context = self._build_context(sources)
        
        # Ù…Ø±Ø­Ù„Ù‡ 4: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
        generation_start = time.time()
        prompt = self._build_prompt(query, context, query_type)
        
        response = self.gemini_model.generate_content(prompt)
        answer = response.text
        
        generation_time = time.time() - generation_start
        
        # Ù…Ø±Ø­Ù„Ù‡ 5: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø¹ØªÙ…Ø§Ø¯
        confidence = self._calculate_confidence(sources, answer)
        
        total_time = time.time() - start_time
        
        return RAGResponse(
            query=query,
            answer=answer,
            sources=sources,
            confidence=confidence,
            generation_time=generation_time,
            retrieval_time=retrieval_time,
            total_time=total_time,
            query_type=query_type,
            document_types=[s.get('type', 'unknown') for s in sources]
        )
    
    def _classify_query(self, query):
        """Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†ÙˆØ¹ Ù¾Ø±Ø³Ø´"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['Ú†Ú¯ÙˆÙ†Ù‡', 'Ø±ÙˆØ´', 'Ø·Ø±ÛŒÙ‚Ù‡']):
            return 'procedure'
        elif any(word in query_lower for word in ['Ú†ÛŒØ³Øª', 'ØªØ¹Ø±ÛŒÙ', 'Ù…Ø¹Ù†ÛŒ']):
            return 'definition'
        elif any(word in query_lower for word in ['Ù…Ø«Ø§Ù„', 'Ù†Ù…ÙˆÙ†Ù‡']):
            return 'example'
        else:
            return 'general'
    
    def _build_context(self, sources):
        """Ø³Ø§Ø®Øª Context Ø¨Ø±Ø§ÛŒ LLM"""
        context = "Ù…Ù†Ø§Ø¨Ø¹ ÛŒØ§ÙØª Ø´Ø¯Ù‡:\n\n"
        
        for i, source in enumerate(sources, 1):
            context += f"Ù…Ù†Ø¨Ø¹ {i}:\n"
            context += f"ÙØ§ÛŒÙ„: {source['source']}\n"
            context += f"Ù…ØªÙ†:\n{source['text']}\n"
            context += f"Ø§Ø¹ØªÙ…Ø§Ø¯: {source['confidence']:.2%}\n"
            context += "-" * 50 + "\n"
        
        return context
    
    def _build_prompt(self, query, context, query_type):
        """Ø§ÛŒØ¬Ø§Ø¯ Prompt Ø¨Ø±Ø§ÛŒ LLM"""
        return f"""
Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø³Ù†Ø§Ø¯ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø¨ÛŒ Ù‡Ø³ØªÛŒØ¯.

Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±:
{query}

Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø±ØªØ¨Ø·:
{context}

Ù„Ø·ÙØ§Ù‹:
1. Ù¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„ Ùˆ Ø¯Ù‚ÛŒÙ‚ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯
2. Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯
3. Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹ Ú©Ø§ÙÛŒ Ù†Ø¯Ø§Ø±ÛŒØ¯ØŒ Ø¨Ú¯ÙˆÛŒÛŒØ¯

Ù¾Ø§Ø³Ø®:
"""
```

### 5. Ú©Ø¯ Flask App

```python
from flask import Flask, request, jsonify
from flask_socketio import SocketIO
from dms_rag_system import DMSRAGSystem

app = Flask(__name__)
socketio = SocketIO(app, cors_allowed_origins="*")

# Ù…Ù‚Ø¯Ø§Ø±â€ŒØ¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø³Ø§Ù…Ø§Ù†Ù‡
dms_rag = None

def initialize_dms_rag():
    """Ù…Ù‚Ø¯Ø§Ø±â€ŒØ¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ RAG"""
    global dms_rag
    try:
        dms_rag = DMSRAGSystem(
            index_dir="./water_index",
            env_path="./.env"
        )
        print("âœ… Ø³Ø§Ù…Ø§Ù†Ù‡ RAG Ù…Ù‚Ø¯Ø§Ø±â€ŒØ¯Ù‡ÛŒ Ø´Ø¯")
        return True
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù…Ù‚Ø¯Ø§Ø±â€ŒØ¯Ù‡ÛŒ: {e}")
        return False

@app.route('/api/chat', methods=['POST'])
def chat():
    """Ù†Ù‚Ø·Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ Ø§ØµÙ„ÛŒ Ú†Øª"""
    if dms_rag is None:
        return jsonify({'error': 'Ø³Ø§Ù…Ø§Ù†Ù‡ Ù‡Ù†ÙˆØ² Ø¢Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³Øª'}), 503
    
    user_query = request.json.get('message')
    
    try:
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø±Ø³Ø´
        response = dms_rag.process_query(user_query)
        
        return jsonify({
            'answer': response.answer,
            'sources': [
                {
                    'text': s['text'][:200],
                    'source': s['source'],
                    'confidence': s['confidence']
                }
                for s in response.sources
            ],
            'confidence': response.confidence,
            'query_type': response.query_type,
            'response_time': response.total_time
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@socketio.on('message')
def handle_socket_message(data):
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Real-time"""
    message = data.get('message')
    response = dms_rag.process_query(message)
    
    socketio.emit('response', {
        'answer': response.answer,
        'sources': response.sources,
        'confidence': response.confidence
    })

if __name__ == '__main__':
    initialize_dms_rag()
    socketio.run(app, host='0.0.0.0', port=5000, debug=True)
```

---

## ÙØ±Ø¢ÛŒÙ†Ø¯ Ø´Ø§Ø®Øµâ€ŒØ³Ø§Ø²ÛŒ

### Ù…Ø±Ø§Ø­Ù„ Ø¯Ù‚ÛŒÙ‚:

```
1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³Ù†Ø¯ (file_path)
   â†“
2. Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ OCR
   â†“
3. ØªÙ‚Ø³ÛŒÙ… Ù…Ø¹Ù†Ø§ÛŒÛŒ (Smart Chunking)
   â†“
4. ØªÙˆÙ„ÛŒØ¯ Embeddingâ€ŒÙ‡Ø§
   â†“
5. Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø§Ø®Øµ FAISS
   â†“
6. Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Metadata Ùˆ Index
```

### Ù†Ù…ÙˆÙ†Ù‡ Ø®Ø±ÙˆØ¬ÛŒ:

```json
{
  "dms_faiss_index.index": "Ø´Ø§Ø®Øµ FAISS (Ø¨Ø§ÛŒÙ†Ø§Ø±ÛŒ)",
  "dms_metadata.json": [
    {
      "text": "Ø¨Ø®Ø´ Ù…ØªÙ† Ø³Ù†Ø¯...",
      "chunk_id": "chunk_00001",
      "source_file": "water_document/water1.md",
      "chunk_index": 1,
      "metadata": {
        "document_title": "Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù†Ø§Ø¨Ø¹",
        "content_type": "technical",
        "keywords": ["SCADA", "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ", "DMS"]
      }
    }
  ],
  "dms_index_summary.json": {
    "total_documents": 5,
    "total_chunks": 250,
    "embedding_model": "intfloat/multilingual-e5-large",
    "embedding_dimension": 1024,
    "indexed_at": "2025-12-10T12:34:56"
  }
}
```

---

## Ø³ÛŒØ³ØªÙ… Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

### Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø±ÛŒØ§Ø¶ÛŒ:

#### Ø§Ù„Ù) Embedding Ùˆ FAISS

```
query_embedding: q âˆˆ â„Â¹â°Â²â´

similarity(q, d) = 1 - ||q - d|| / max_distance
                 âˆˆ [0, 1]

top_k = argmax_k(similarity(q, d_i))
```

#### Ø¨) BM25

```
BM25(D, Q) = Î£(IDF(q_i) * (f(q_i, D) * (k1 + 1)) / 
             (f(q_i, D) + k1 * (1 - b + b * |D| / avgdl)))

IDF(q) = log((N - n(q) + 0.5) / (n(q) + 0.5))
```

#### Ø¬) RRF

```
score(d) = Î£(1 / (k + rank(d, S)))
k = 60
```

### Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ:

```
Query: "Ú†Ú¯ÙˆÙ†Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ SCADA Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ØŸ"

Ù…Ø±Ø­Ù„Ù‡ 1: Embedding
â†’ Vector 1024-Ø¨Ø¹Ø¯ÛŒ

Ù…Ø±Ø­Ù„Ù‡ 2: Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
   FAISS: ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ = [0.15, 0.22, 0.31, 0.45, 0.68]
   scores = [0.85, 0.78, 0.69, 0.55, 0.32]
   
   Ranking: [doc_1(0.85), doc_5(0.78), doc_3(0.69), ...]

Ù…Ø±Ø­Ù„Ù‡ 3: Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„ÛŒØ¯ÛŒ
   BM25: scores = [8.5, 6.2, 4.1, 2.3, 1.5]
   
   Ranking: [doc_1(8.5), doc_3(6.2), doc_5(4.1), ...]

Ù…Ø±Ø­Ù„Ù‡ 4: ØªØ±Ú©ÛŒØ¨ RRF
   doc_1: 1/61 + 1/61 = 0.0328
   doc_3: 1/63 + 1/62 = 0.0318
   doc_5: 1/62 + 1/63 = 0.0317
   
   Final Ranking: [doc_1, doc_3, doc_5, ...]

Ù†ØªÛŒØ¬Ù‡: 5 Ø¨Ù‡ØªØ±ÛŒÙ† Ø³Ù†Ø¯ Ù…Ø±ØªØ¨Ø·
```

---

## ÙˆØ§Ø³Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ùˆ API

### Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ API:

#### 1. Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú†Øª

```bash
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Ú†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ù†Ø±Ú˜ÛŒ Ø±Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø±Ø¯ØŸ"
  }'
```

#### 2. Ù¾Ø§Ø³Ø®

```json
{
  "answer": "Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³Ù†Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†Ø±Ú˜ÛŒ Ø¯Ø± Ø³Ø§Ù…Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµÙÛŒÙ‡ Ø¢Ø¨ Ø´Ø§Ù…Ù„ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø§Ø³Øª:\n\n1. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§\n2. Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø±Ø¹Øª Ù¾Ù…Ù¾Ø§Ú˜\n3. Ù…Ø¯ÛŒØ±ÛŒØª ØªÙˆØ§Ù„ÛŒ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§\n\nØ§ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ ØªØ§ 25-30 Ø¯Ø±ØµØ¯ Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ Ø±Ø§ Ú©Ø§Ù‡Ø´ Ø¯Ù‡Ù†Ø¯.",
  
  "sources": [
    {
      "text": "Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ§Ø³ÛŒØ³Ø§Øª ØªØµÙÛŒÙ‡ Ø¢Ø¨... Ø¯Ø± ØªØµÙÛŒÙ‡â€ŒØ®Ø§Ù†Ù‡ Createch360 Ø¯Ø± Ø§ÛŒØªØ§Ù„ÛŒØ§ 19 Ø¯Ø±ØµØ¯ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ Ø«Ø¨Øª Ø´Ø¯",
      "source": "water_document/WaterOptimizing.md",
      "confidence": 0.92
    },
    {
      "text": "Ú©Ø§Ù‡Ø´ 25 Ø¯Ø±ØµØ¯ Ù…ØµØ±Ù Ø§Ù†Ø±Ú˜ÛŒ Ù¾Ù…Ù¾â€ŒÙ‡Ø§ØŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ 30 Ø¯Ø±ØµØ¯ ØªØ®ØµÛŒØµ Ù†ÛŒØ±ÙˆÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ...",
      "source": "water_document/water1.md",
      "confidence": 0.85
    }
  ],
  
  "confidence": 0.88,
  "query_type": "procedure",
  "response_time": 2.34
}
```

### Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ WebSocket:

```javascript
// Frontend Code
const socket = io('http://localhost:5000');

socket.emit('message', {
  message: 'Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³Ø§Ù…Ø§Ù†Ù‡ Ú†ÛŒØ³ØªØŸ'
});

socket.on('response', (data) => {
  console.log('Ù¾Ø§Ø³Ø®:', data.answer);
  console.log('Ù…Ù†Ø§Ø¨Ø¹:', data.sources);
});
```

---

## Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡

### Ù†Ù…ÙˆÙ†Ù‡ 1: Ù¾Ø±Ø³Ø´ Ø³Ø§Ø¯Ù‡

```
User: "SCADA Ú†ÛŒØ³ØªØŸ"

System Process:
1. Query Type: definition
2. Retrieved: 3 sources
3. Confidence: 0.91
4. Response Time: 0.87s

Answer:
SCADA (Supervisory Control and Data Acquisition) ÛŒØ§ Ø³Ø§Ù…Ø§Ù†Ù‡ Ù†Ø¸Ø§Ø±Øª Ùˆ Ú©Ù†ØªØ±Ù„ 
Ùˆ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø³Øª Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ø¸Ø§Ø±Øª Ùˆ Ú©Ù†ØªØ±Ù„ ÙØ±Ø¢ÛŒÙ†Ø¯Ù‡Ø§ÛŒ 
ØµÙ†Ø¹ØªÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø¯Ø± Ø³Ø§Ù…Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø¨ÛŒØŒ SCADA Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ù„ ØªØµÙÛŒÙ‡â€ŒØ®Ø§Ù†Ù‡â€ŒÙ‡Ø§ØŒ 
Ù¾Ù…Ù¾â€ŒÙ‡Ø§ Ùˆ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆØ²ÛŒØ¹ Ø¢Ø¨ Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯.
```

### Ù†Ù…ÙˆÙ†Ù‡ 2: Ù¾Ø±Ø³Ø´ Ù¾ÛŒÚ†ÛŒØ¯Ù‡

```
User: "Ú†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ø³Ø¯ Ø®Ø§Ú©ÛŒ Ø±Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø±Ø¯ØŸ"

System Process:
1. Query Type: procedure
2. Retrieved: 5 sources
3. Confidence: 0.82
4. Response Time: 3.21s

Answer:
Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ø³Ø¯ Ø®Ø§Ú©ÛŒ Ø´Ø§Ù…Ù„ Ù…Ø±Ø§Ø­Ù„ Ø²ÛŒØ± Ø§Ø³Øª:

1. Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù†Ø§Ø¨Ø¹ Ù‡ÙˆØ´Ù…Ù†Ø¯:
   - ØªØ®ØµÛŒØµ Ø¨Ù‡ÛŒÙ†Ù‡ Ù†ÛŒØ±ÙˆÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‡Ø§Ø±Øª
   - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ù…Ø§Ø´ÛŒÙ†â€ŒØ¢Ù„Ø§Øª
   - Ú©Ø§Ù‡Ø´ Ø²Ù…Ø§Ù† Ø®Ø§Ù„ÛŒ

2. Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø¨Ù‡ØªØ±:
   - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ MILP Ø¨Ø±Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ
   - Ø§ÙØ²Ø§ÛŒØ´ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ§Ø²ÛŒ
   - Ú©Ø§Ù‡Ø´ ØªØ£Ø®ÛŒØ±Ø§Øª

3. Ù…Ø¯ÛŒØ±ÛŒØª Ù…ØµØ§Ù„Ø­:
   - Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø¯Ù‚ÛŒÙ‚ ØªØ§Ù…ÛŒÙ†
   - Ú©Ø§Ù‡Ø´ Ù‡Ø¯Ø±Ø±ÙØª
   - Ø®Ø±ÛŒØ¯ Ø¨Ø±ÙˆÙ‚Øª Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ø² Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ØªØ±

Ù…Ù†Ø§Ø¨Ø¹:
- WaterOptimizing.md: Ø¨Ø®Ø´ "Ø®Ø±ÙˆØ¬ÛŒ Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ" (92% Ø§Ø·Ù…ÛŒÙ†Ø§Ù†)
- water1.md: Ø¨Ø®Ø´ "Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒÛŒ" (87% Ø§Ø·Ù…ÛŒÙ†Ø§Ù†)
```

### Ù†Ù…ÙˆÙ†Ù‡ 3: Ù¾Ø±Ø³Ø´ Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡ Ù…ØªÙ† Ù†Ø¯Ø§Ø±Ø¯

```
User: "Ù‡ÙˆØ§ÛŒ Ú©Ø±Ø¬ Ú†Ú¯ÙˆÙ†Ù‡ Ø§Ø³ØªØŸ"

System Response:
Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ØŒ Ø§ÛŒÙ† Ù¾Ø±Ø³Ø´ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø§Ø³Ù†Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª. 
Ø§Ø³Ù†Ø§Ø¯ Ù…Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø¨ÛŒ Ø®Ø±Ø§Ø³Ø§Ù† Ø±Ø¶ÙˆÛŒ Ù…ØªÙ…Ø±Ú©Ø² Ù‡Ø³ØªÙ†Ø¯.

Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ù…Ù† Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¨Ù¾Ø±Ø³ÛŒØ¯:
- Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø¨ÛŒ Ùˆ Ø³Ø¯Ù‡Ø§
- Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù†Ø§Ø¨Ø¹ Ø§Ù†Ø³Ø§Ù†ÛŒ Ùˆ Ù…Ø§Ø´ÛŒÙ†â€ŒØ¢Ù„Ø§Øª
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø²ÛŒÙ†Ù‡ Ùˆ Ø²Ù…Ø§Ù†
- ØªØµÙÛŒÙ‡â€ŒØ®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø¨
```

---

## Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙÙ†ÛŒ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ

### Ø¹Ù…Ù„Ú©Ø±Ø¯:

```
Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ:
- Ø§Ø³Ù†Ø§Ø¯: 1-10,000+
- Chunks: 100-1,000,000+
- Ø²Ù…Ø§Ù† Ø¬Ø³ØªØ¬Ùˆ: 50-200ms
- Ø²Ù…Ø§Ù† ØªÙˆÙ„ÛŒØ¯: 1-5 Ø«Ø§Ù†ÛŒÙ‡

Ø­Ø§ÙØ¸Ù‡:
- Embedding Model: 4-8 GB
- FAISS Index: 0.5-2 GB (Ø¨Ø³ØªÙ‡ Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ vectors)
- Metadata: 10-100 MB
```

### Ú©ÛŒÙÛŒØª:

```
Ù†Ø±Ø® Ø¯Ù‚Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ:
- Semantic Search (FAISS): 85-95%
- Keyword Search (BM25): 70-85%
- Hybrid (RRF): 90-98%

Ø§Ø¹ØªÙ…Ø§Ø¯ Ù¾Ø§Ø³Ø®:
- High (>0.85): Ø®ÛŒÙ„ÛŒ Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯
- Medium (0.70-0.85): ØªØ§ Ø­Ø¯ÙˆØ¯ÛŒ Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯
- Low (<0.70): Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ£ÛŒÛŒØ¯
```

---

## Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ

Ø§ÛŒÙ† Ø³Ø§Ù…Ø§Ù†Ù‡ RAG ÛŒÚ© Ù¾Ù„ØªÙØ±Ù… Ø¬Ø§Ù…Ø¹ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø³Ù†Ø§Ø¯ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø¨ÛŒ Ø§Ø³Øª Ú©Ù‡:

âœ… **Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ** - Ø¯Ø±Ú© ÙˆØ§Ù‚Ø¹ÛŒ Ù…Ø¹Ù†ÛŒ Ù¾Ø±Ø³Ø´â€ŒÙ‡Ø§
âœ… **Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ù‚ÛŒÙ‚** - ÛŒØ§ÙØªÙ† Ø§Ø³Ù†Ø§Ø¯ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ø§Ù„Ø§
âœ… **Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯** - ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØªÙˆØ¶ÛŒØ­ÛŒ Ùˆ Ù…ÙÛŒØ¯
âœ… **Ù…Ù†Ø§Ø¨Ø¹ Ø´ÙØ§Ù** - Ø§Ø±Ø§Ø¦Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù¾Ø§Ø³Ø®
âœ… **Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ** - ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ú©Ø§Ø± Ø¨Ø§ Ù‡Ø²Ø§Ø±Ø§Ù† Ø³Ù†Ø¯

---

## Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡

### Ø´Ø±ÙˆØ¹ Ø³ÛŒØ³ØªÙ…:

```bash
# 1. Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
pip install -r requirements.txt

# 2. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·
export GOOGLE_API_KEY="your-api-key"

# 3. Ø³Ø§Ø®Øª Ø´Ø§Ø®Øµ
python create_dms_index.py

# 4. Ø´Ø±ÙˆØ¹ Ø³Ø±ÙˆØ±
python app.py
```

### Ø¯Ø³ØªØ±Ø³ÛŒ:

- **ÙˆØ¨**: http://localhost:5000
- **API**: http://localhost:5000/api/chat
- **WebSocket**: ws://localhost:5000

---

**Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡:** 2025-12-10
**Ù†Ø³Ø®Ù‡:** 1.0
**ÙˆØ¶Ø¹ÛŒØª:** ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù‡
