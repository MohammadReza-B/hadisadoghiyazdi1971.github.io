---
layout: persian  # یا single با کلاس rtl-layout
classes: wide rtl-layout
dir: rtl
title: "تحلیل واریانس برآوردگرهای اعتبارسنجی متقابل خطای تعمیم "
permalink: /teaching/studenteffort/patterneffort/Analysis_of_Variance_of_Cross_Validation_Estimators_of_the_Generalization_Error/
author_profile: true
sidebar:
  nav: "patterneffort"
header:
  overlay_image: "/assets/images/background.jpg"
  overlay_filter: 0.3
  overlay_color: "#5e616c"
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
---

**[بسم‌الله الرحمن الرحیم]**

**[عنوان: تحلیل واریانس برآوردگرهای اعتبارسنجی متقابل خطای تعمیم]**

**[دانشگاه فردوسی مشهد - دانشکده مهندسی - گروه کامپیوتر]**

**[رشته: کارشناسی ارشد هوش مصنوعی]**

**[استاد راهنما: دکتر هادی صدوقی یزدی]**

**[نام دانشجو: اسماعیل برزگری]**

**[تاریخ: پاییز- 1404]**

**[آدرس ایمیل:]** smailbarzegari@gmail.com

---

# 1. مقدمه

همانطور که در بخش اول ارائه مطرح کردیم ما برای اعتبارسنجی داده ها آنها را به دو بخش آموزشی و آزمایشی تقسیم می کنیم.
ما در آن بخش به 5 نوع تقسیم داده اشاره کردیم و برای هر یک، قطعه برنامه اجرایی ارائه دادیم.

در این بخش به روش‌هایی از دو رشته مختلف آمار و یادگیری ماشین رااشاره خواهیم کرد.
ما به مسئله تخمین واریانس تخمین‌گرهای اعتبارسنجی متقابل (CV) خطای تعمیم می‌پردازیم. به‌طور خاص، مسئله تخمین واریانس تخمین‌گرهای CV خطای تعمیم را به‌عنوان مسئله‌ای در تقریب گشتاورهای یک آماره بررسی می‌کنیم. این تقریب نقش مجموعه‌های آموزش و آزمون را در عملکرد الگوریتم نشان می‌دهد. این رویکرد یکپارچه‌ای برای ارزیابی روش‌های مختلف مورد استفاده در به‌دست آوردن مجموعه‌های آموزش و آزمون ارائه می‌دهد و تغییرپذیری ناشی از مجموعه‌های آموزش و آزمون مختلف را در نظر می‌گیرد. برای مسئله ساده پیش‌بینی میانگین نمونه و در مورد توابع زیان هموار، نشان می‌دهیم که واریانس تخمین‌گر CV خطای تعمیم تابعی از گشتاورهای متغیرهای تصادفی 
$$( Y = \text{Card}(S_j \cap S_i) ),( Y^* = \text{Card}(S_j' \cap S_i') )\$$
است، که در آن 
$$( S_j,S_i )\$$
دو مجموعه آموزش و 
$$( S_j',S_i' )$$
مجموعه‌های آزمون متناظر هستند. ما ثابت می‌کنیم که توزیع 
$$( Y ),( Y^* )\$$
 فوق‌هندسی است و تخمینگر خود را با تخمینگر پیشنهادشده توسط Nadeau و Bengio (2003) مقایسه می‌کنیم. ما این نتایج را برای مورد رگرسیون و تابع زیان خطای مطلق گسترش می‌دهیم و نشان می‌دهیم که چگونه می‌توان روش‌ها را به مورد طبقه‌بندی تعمیم داد. ما نتایج را از طریق شبیه‌سازی نشان می‌دهیم.

[Link 1](https://www.jmlr.org/papers/volume6/markatou05a/markatou05a.pdf)

خطای تعمیم، خطایی است که مدل ما در مواجهه با داده‌های جدید و دیده نشده مرتکب می‌شود. این خطا معیاری از توانایی مدل برای "تعمیم" یافته‌هایش به دنیای واقعی است. هدف نهایی ما در یادگیری ماشین، کمینه کردن این خطاست.

[Link 2](deepseek)

خطای تعمیم یک روش یادگیری را می‌توان به راحتی از طریق اعتبارسنجی متقابل تخمین زد. با این حال، ارائه یک تخمین واریانس برای تخمینگر این خطای تعمیم مسئله دشوارتری است. این به دلیل وابستگی خطای تعمیم به تابع زیان درگیر است و ریاضیات مورد نیاز برای تحلیل واریانس تخمینگر پیچیده است. یک تخمینگر برای واریانس تخمینگر اعتبارسنجی متقابل خطای تعمیم توسط Nadeau و Bengio (2003) پیشنهاد شده است. در بخش بعدی این تخمینگر را بحث کرده و آن را با تخمینگر جدید پیشنهادی مقایسه خواهیم کرد.

در این بخش، به طور خلاصه کارهای مرتبط را مرور می‌کنیم و نمادگذاری مورد استفاده در این مقاله را تنظیم می‌کنیم.
علاقه به مسئله تقسیم‌بندی «بهینه» یک نمونه ثابت n در دو مجموعه، آموزش و آزمایش، به سال ۱۹۶۲ برمی‌گردد. هایلیمن (۱۹۶۲) این مسئله را در زمینه طراحی یک ماشین تشخیص الگو بررسی کرد و تقسیم‌بندی «بهینه» کل نمونه را به عنوان تقسیم‌بندی‌ای تعریف کرد که واریانس یک تخمین‌گر بی‌طرف از احتمال خطای ماشین تشخیص الگو را به حداقل می‌رساند.
هایلیمن (۱۹۶۲) بیان می‌کند که همین قانون، یعنی به حداقل رساندن واریانس خطای یک قانون طبقه‌بندی، می‌تواند با تخمین‌گرهای خطایی که می‌توانند به دلیل بایاس اصلاح شوند، استفاده شود. کار مشابهی توسط لارسن و گوت (۱۹۹۹) انجام شده است.
این نویسندگان به مسئله تقسیم یک نمونه ثابت n به یک مجموعه آموزشی، یک مجموعه آزمایشی و یک مجموعه اعتبارسنجی با استفاده از معیار میانگین مربعات خطا می‌پردازند. فقط خطای مربعات زیان در حالت ساده تخمین پارامتر مکان مورد مطالعه قرار می‌گیرد. روش‌های اعتبارسنجی متقابل که آنها مطالعه می‌کنند، اعتبارسنجی متقابل hold-out، k-fold و اعتبارسنجی متقابل randomation permutation هستند. 

[Link 3](https://arxiv.org/pdf/1511.02980)

**کلیدواژه‌ها**: اعتبارسنجی متقابل، خطای تعمیم، تقریب گشتاور، پیش‌بینی، تخمین واریانس

---

# ۲. چارچوب و کارهای مرتبط

## ۲.۱. چارچوب و تخمینگر اعتبارسنجی متقابل خطای تعمیم
فرض کنید داده‌های 
$$(X_1, X_2, \cdots, X_n)$$
جمع‌آوری شده‌اند به طوری که جهان داده، 
$$(Z_1^n = \{X_1, X_2, \cdots, X_n\})$$
، مجموعه‌ای از مشاهدات مستقل و هم‌توزیع است که از یک توزیع احتمال ناشناخته، که با \(F\) نشان داده می‌شود، پیروی می‌کنند. اجازه دهید \(S\) زیرمجموعه‌ای با اندازه 
$$(n_1)، (n_1 < n)$$
، گرفته شده از 
$$(Z_1^n)$$
را نشان دهد. این زیرمجموعه از مشاهدات، مجموعه آموزش نامیده می‌شود. بر اساس یک مجموعه آموزش، یک قاعده ساخته می‌شود. مجموعه آزمون شامل تمام داده‌هایی است که به \(S\) تعلق ندارند، یعنی مجموعه آزمون مجموعه 
$$(S^c = Z_1^n \setminus S)$$
، مکمل \(S\) نسبت به جهان داده 
$$(Z_1^n)$$
است. اندازه یک مجموعه آزمون را با 
$$(n_2)$$
نشان دهید، 
$$(n_2 = n - n_1)،(n_2 < n)$$

فرض کنید
$$(L: \mathbb{R}^p \times \mathbb{R} \rightarrow \mathbb{R})$$
یک تابع باشد، و فرض کنید \(Y\) یک متغیر هدف و 
$$(\hat{f}(x))$$
یک قاعده تصمیم است. تابع 
$$(L(Y, \hat{f}(X)))$$
که خطای بین متغیر هدف و قاعده پیش‌بینی را اندازه می‌گیرد، تابع زیان نامیده می‌شود.

به عنوان مثال، تخمین میانگین نمونه را در نظر بگیرید. در این مسئله، الگوریتم یادگیری از 
$$(\hat{f}(x) = \frac{1}{n_1} \sum_{i=1}^{n_1} X_i = \bar{X}_{S_j})$$
به عنوان یک قاعده تصمیم و 
$$(L(\bar{X}_{S_j}, X_i) = (\bar{X}_{S_j} - X_i)^2)،(X_i \in S_j^c)$$
، یعنی زیان مربعات خطا، به عنوان تابع زیان استفاده می‌کند. انتخاب‌های معمول دیگر تابع زیان شامل زیان خطای مطلق، 
$$(|\bar{X}_{S_j} - X_i|)$$
و تابع زیان ۰-۱ است که عمدتاً در طبقه‌بندی استفاده می‌شود.

نتایج ما تغییرپذیری در هر دو مجموعه آموزش و آزمون را در نظر می‌گیرد. تخمین واریانس تخمینگر اعتبارسنجی متقابل خطای تعمیم را می‌توان تحت طرح‌های اعتبارسنجی متقابل زیر محاسبه کرد. اولی چیزی است که ما آن را **انتخاب تصادفی کامل** می‌نامیم. هنگامی که این شکل از اعتبارسنجی متقابل برای محاسبه تخمین خطای تعمیم یک روش یادگیری استفاده می‌شود، مجموعه‌های آموزش، و در نتیجه مجموعه‌های آزمون، به طور تصادفی از جهان داده موجود انتخاب می‌شوند. در مورد **انتخاب مجموعه آزمون غیرهمپوشان**، جهان داده به k زیرمجموعه داده غیرهمپوشان تقسیم می‌شود. سپس هر زیرمجموعه داده به عنوان یک مجموعه آزمون استفاده می‌شود، در حالی که داده‌های باقی‌مانده به عنوان مجموعه آموزش عمل می‌کنند. این مورد اعتبارسنجی متقابل k-دسته‌ای است.

اکنون به طور مفصل تخمینگر اعتبارسنجی متقابل خطای تعمیم را که واریانس آن را مطالعه خواهیم کرد، توصیف می‌کنیم. این تخمینگر تحت مورد انتخاب تصادفی کامل ساخته شده است.

فرض کنید \(A_j\) یک مجموعه تصادفی از \(n_1\) عدد صحیح متمایز از 
$$(\{1, 2, \cdots, n\})$$
باشد، 
$$(n_1 < n)$$
اجازه دهید 
$$(n_2 = n - n_1)$$
اندازه مجموعه مکمل متناظر باشد. توجه کنید که در اینجا \(n_2\) یک عدد ثابت است و 
$$(Card(A_j) = n_1)$$
ثابت است. اجازه دهید 
$$(A_1, A_2, \cdots, A_J)$$
مجموعه‌های اندیس تصادفی باشند که به طور مستقل از یکدیگر نمونه‌گیری شده‌اند و با 
$$(A_j^c)$$
مکمل 
$$(A_j)،(j = 1, 2, \cdots, J)$$
را نشان دهید. همچنین با 
$$(S_j = \{X_l : l \in A_j\})،(j = 1, 2, \cdots, J)$$
نشان دهید. این مجموعه آموزش است که با زیرنمونه‌گیری از 
$$(Z_1^n)$$
مطابق با مجموعه اندیس تصادفی \(A_j\) به دست می‌آید. سپس مجموعه آزمون متناظر 
$$(S_j^c = \{X_l : l \in A_j^c\})$$
است. اکنون 
$$(L(j, i) = L(S_j, X_i))$$
را تعریف کنید، که در آن \(L\) یک تابع زیان است. توجه کنید که \(L\) با وابستگی آن به مجموعه آموزش \(S_j\) و مجموعه آزمون \(S_j^c\) تعریف شده است. این وابستگی به مجموعه‌های آموزش و آزمون از طریق آماره‌هایی است که با استفاده از عناصر این مجموعه‌ها محاسبه می‌شوند. میانگین خطای مجموعه آزمون معمول آنگاه به صورت زیر است:

$$\hat{\mu}_j = \frac{1}{n_2} \sum_{i \in S_j^c} L(j, i)$$
,tag{2.1}


تخمینگر اعتبارسنجی متقابل که مطالعه خواهیم کرد به صورت زیر تعریف می‌شود:

$${}^{n_2}_{n_1}\hat{\mu}_J = \frac{1}{J} \sum_{j=1}^{J} \hat{\mu}_j$$
,tag{2.2}


این نسخه از تخمینگر اعتبارسنجی متقابل خطای تعمیم به مقدار \(J\)، اندازه مجموعه‌های آموزش و آزمون و اندازه جهان داده بستگی دارد. این تخمینگر توسط Nadeau و Bengio (2003) مطالعه شده است. این نویسندگان دو تخمینگر برای واریانس 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
 ارائه کردند. در بخش بعدی به طور خلاصه تخمینگرهای ارائه شده توسط Nadeau و Bengio (2003) و همچنین سایر کارها در این موضوع را مرور می‌کنیم. در بخش بعدی خواهیم دید که وقتی \(J\) به طور مناسب انتخاب شود، تخمینگر Nadeau و Bengio (2003) در برخی از مواردی که مطالعه می‌کنیم نزدیک به تخمینگر تقریب گشتاور عمل کرده و عملکرد مشابهی دارد.

## ۲.۲. کارهای مرتبط
ادبیات مرتبط برای مسئله تخمین واریانس خطای تعمیم شامل کارهای McLachlan (1972, 1973, 1974, 1976) و کارهای Nadeau و Bengio (2003) و Bengio و Grandvalet (2004) است. در اینجا، به طور خلاصه این کارها را مرور می‌کنیم.

فرض کنید
$$(S^2_{\hat{\mu}_j} = \frac{1}{J-1} \sum_{j=1}^{J} (\hat{\mu}_j - {}^{n_2}_{n_1}\hat{\mu}_J)^2)$$
واریانس نمونه 
$$(\hat{\mu}_j)،(j = 1, 2, \cdots, J)\$$
باشد. سپس Nadeau و Bengio (2003) نشان می‌دهند که:

$$E(S_{\hat{\mu}_j}^2) = \frac{Var({}^{n_2}_{n_1}\hat{\mu}_J)}{(\frac{1}{J} + \frac{\rho}{1 - \rho})}$$
,tag{2.3}

که در آن \(\rho\) همبستگی بین 
$$(\hat{\mu}_j),(\hat{\mu}_{j'})$$
است. بنابراین، اگر \(\rho\) شناخته شده باشد،

$$(\frac{1}{J} + \frac{\rho}{1 - \rho}) S_{\hat{\mu}_j}^2$$
,tag{2.4}

یک تخمینگر بدون بایاس برای 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
است. Nadeau و Bengio (2003) مشاهده می‌کنند که این تخمینگر به همبستگی \(\rho\) بین 
$$(\hat{\mu}_j)$$
های مختلف بستگی دارد که تخمین آن دشوار است. بنابراین، آنها یک تقریب برای همبستگی پیشنهاد می‌کنند، 
$$(\hat{\rho} = \frac{n_2}{n})$$
، که در آن \(n_2\) اندازه مجموعه آزمون است. تخمینگر نهایی واریانس 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
به صورت زیر داده می‌شود:

$$(\frac{1}{J} + \frac{n_2}{n_1}) S_{\hat{\mu}_j}^2$$
,tag{2.5}

Nadeau و Bengio (2003) خاطرنشان می‌کنند که تخمینگر پیشنهادی فوق ساده است اما ممکن است نسبت به 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
واقعی دارای اریبی مثبت یا منفی باشد. یعنی تمایل دارد 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
را بیش‌تخمین یا کم‌تخمین بزند بسته به اینکه 
$$(\hat{\rho} = \frac{n_2}{n} > \rho) or (\hat{\rho} < \rho)$$
باشد. بنابراین، این تخمینگر دقیقاً بی‌طرف نیست.

Nadeau و Bengio (2003) همچنین تخمینگر دیگری برای واریانس تخمینگر اعتبارسنجی متقابل خطای تعمیم پیشنهاد کردند. این تخمینگر بدون بایاس است اما 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
را بیش‌تخمین می‌زند. به صورت زیر محاسبه می‌شود: فرض کنید \(n\) اندازه جهان داده باشد و بدون از دست دادن کلیت، فرض کنید \(n\) زوج است. مجموعه داده را به طور تصادفی به دو زیرمجموعه داده با اندازه مساوی تقسیم کنید. سپس تخمینگر اعتبارسنجی متقابل خطای تعمیم را روی این دو زیرمجموعه داده محاسبه کنید. توجه کنید که اندازه مجموعه آموزش اکنون 
$$(n'_1 = [\frac{n}{2}] - n_2 < n_1)$$
است، کوچکتر از اندازه اصلی مجموعه آموزش، اما اندازه مجموعه آزمون ثابت می‌ماند. با $$(\hat{\mu}_1)$$
تخمینگر 
$$({}^{n_2}_{n'_1}\hat{\mu}_J)$$
محاسبه شده روی زیرمجموعه داده اول و با 
$$(\hat{\mu}_2)$$
تخمینگر 
$$({}^{n_2}_{n'_1}\hat{\mu}_J)$$
محاسبه شده روی زیرمجموعه داده دوم را نشان دهید. برای به دست آوردن یک تخمینگر برای واریانس تخمینگر اعتبارسنجی متقابل خطای تعمیم، واریانس نمونه 
$$(\hat{\mu}_1),(\hat{\mu}_2)$$
را محاسبه کنید. فرآیند تقسیم می‌تواند \(M\) بار تکرار شود و Nadeau و Bengio (2003) \(M = 10\) را توصیه می‌کنند. تخمینگر بدون بایاس پیشنهادی سپس به صورت زیر داده می‌شود:

$$\frac{1}{2M} \sum_{m=1}^{M} (\hat{\mu}_{1,m} - \hat{\mu}_{2,m})^2$$
,tag{2.6}

این یک تخمینگر بدون بایاس برای 
$$(Var({}^{n_2}_{n'_1}\hat{\mu}_J))$$
است.

Bengio و Grandvalet (2004) نشان دادند که هیچ تخمینگر بی‌طرف و جهانی برای واریانس اعتبارسنجی متقابل k-دسته‌ای که تحت همه توزیع‌ها معتبر باشد، وجود ندارد. در اینجا، ما تخمینگرهایی برای واریانس تخمینگر اعتبارسنجی متقابل k-دسته‌ای خطای تعمیم به دست می‌آوریم که تقریباً بی‌طرف هستند. با این حال، ما همچنین متوجه می‌شویم که تخمینگرهای ما به توزیع خطاها و دانش الگوریتم یادگیری بستگی دارند.

در یک سری مقالات قابل توجه، McLachlan با توسعه یک تکنیک برای استخراج بسط‌های مجانبی واریانس‌های خطاهای طبقه‌بندی نادرست آماره طبقه‌بندی Anderson، به مسئله تخمین واریانس خطاهای تابع ممیز خطی پرداخت. McLachlan همچنین یک بسط مجانبی از امید ریاضی نرخ خطای تخمین‌زده شده در تحلیل ممیز را ایجاد کرد و توزیع‌های نرخ خطای شرطی و ریسک مرتبط با آماره طبقه‌بندی Anderson را در چارچوب مسئله تمایز دو جامعه به دست آورد. این استخراج‌ها تحت فرض نرمال بودن برای توزیع جامعه انجام شد.

کار ما شباهت‌هایی با کار McLachlan دارد به این معنا که ما تقریب‌هایی برای گشتاورهای توزیع تخمینگر اعتبارسنجی متقابل خطای تعمیم به دست می‌آوریم و از اینها برای به دست آوردن یک تخمینگر واریانس استفاده می‌کنیم. با این حال، ما نرمال بودن مکانیزم زیرین که داده‌ها را تولید کرده است، فرض نمی‌کنیم.

در ادامه، ابتدا روش تقریب گشتاور را برای به دست آوردن یک تخمینگر برای 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
ارائه می‌دهیم. سپس عملکرد این تخمینگر را مطالعه کرده و آن را با تخمینگر Nadeau و Bengio (2003) مقایسه می‌کنیم.

---

# ۳. تخمینگر تقریب گشتاور برای 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$

به یاد بیاورید که 
$$({}^{n_2}_{n_1}\hat{\mu}_J = \frac{1}{J} \sum_{j=1}^{J} \hat{\mu}_j = \frac{1}{J} \sum_{j=1}^{J} (\frac{1}{n_2} \sum_{i \in S_j^c} L(j, i)))$$
. بنابراین 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
یک آماره است. یک تخمینگر برای 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
را می‌توان با تقریب گشتاورهای آماره 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
به دست آورد. یک محاسبه ساده نشان می‌دهد که:

$$Var({}^{n_2}_{n_1}\hat{\mu}_J) = \frac{1}{J^2} \sum_{j=1}^{J} Var(\hat{\mu}_j) + \frac{1}{J^2} \sum \sum_{j \neq j'} Cov(\hat{\mu}_j,\hat{\mu}_{j'})$$
,tag{3.1}

از این فرمول می‌بینیم که اگر بتوانیم دو جمله (3.1) را تقریب بزنیم، آنگاه می‌توانیم یک تخمینگر برای واریانس 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
به دست آوریم. برای دستیابی به این هدف، نیاز داریم که 
$$(E(\hat{\mu}_j)) و (E(\hat{\mu}_j^2)) و (E(\hat{\mu}_j \hat{\mu}_{j'}))$$
را تخمین بزنیم. در بخش‌های بعدی نظریه‌ای را توسعه خواهیم داد که به ما امکان می‌دهد تقریب‌های گشتاور مورد نیاز را به دست آوریم. برای نشان دادن روش‌شناسی به وضوح، مورد تخمین میانگین ساده و مورد رگرسیون را جداگانه درمان می‌کنیم. علاوه بر این، موردی که تابع زیان مشتق‌پذیر است را از مورد توابع زیان غیرقابل مشتق جداگانه درمان می‌کنیم.

## ۳.۱. مورد میانگین نمونه
ما با تحلیل مورد میانگین نمونه شروع می‌کنیم. در اینجا، تابع زیان \(L\) به \(S_j\) از طریق آماری
$$(\bar{X}_{S_j})$$
، میانگین نمونه محاسبه شده با استفاده از عناصر \(S_j\)، و به \(S_j^c\) توسط عناصر \(X_i \in S_j^c\) وابسته است. یکی از دلایل ارائه جداگانه مورد میانگین نمونه این است که به وضوح سهمی را که در تخمینگر 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
به دلیل تغییرپذیری بین مجموعه‌های آموزش و آزمون مختلف وجود دارد، نشان می‌دهد. دلیل دوم به نفع این مورد این است که تحت زیان مربعات خطا، ما یک "استاندارد طلایی" به دست می‌آوریم که در مقابل آن می‌توانیم تخمینگر واریانس تجربی جدید و تخمینگر Nadeau و Bengio (2003) را مقایسه کنیم. این "استاندارد طلایی" مقدار نظری دقیق 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
است. نتایج به دست آمده نشان می‌دهد که تخمینگر واریانس تخمینگر اعتبارسنجی متقابل خطای تعمیم الگوریتم‌هایی که از توابع مشتق‌پذیر میانگین به عنوان توابع زیان استفاده می‌کنند، به امید ریاضی متغیرهای تصادفی 
$$(Y = Card(S_j \cap S_{j'}))$$
 و 
$$(Y^* = Card(S_j^c \cap S_{j'}^c))$$
بستگی دارد.

اجازه دهید تابع زیان 
$$(L(j, i) = L(\bar{X}_{S_j}; X_i))$$
مشتق‌پذیر باشد. در زیر شرایطی را که تحت آن نظریه ما برقرار است، فهرست می‌کنیم.

**فرض ۱**. توزیع 
$$(L(\bar{X}_{S_j}, X_i))$$
به تحقق خاص \(S_j\) و \(i\) بستگی ندارد.

**فرض ۲.** تابع زیان \(L\) به عنوان تابعی از 
$$(\bar{X}_{S_j})$$
به گونه‌ای است که چهار مشتق اول آن نسبت به آرگومان اول برای همه مقادیر متغیری که به \(I\) تعلق دارد وجود دارد، که در آن \(I\) یک بازه است به طوری که 
$$(P(v \in I) = 1)$$
و \(v\) آرگومان اول تابع زیان را نشان می‌دهد.

**فرض ۳.** مشتق چهارم \(L\) به گونه‌ای است که 
$$(|L^{(iv)}(\bar{X}_{S_j}; X_i)| \leq M(X_i))$$
، 
$$(E[M(X_i)] < \infty)$$

فرض ۱ همچنین توسط Nadeau و Bengio (2003, p. 244) استفاده شده است. فرضیات ۲ و ۳ در ادبیاتی که تقریب‌هایی برای گشتاورهای یک تابع پیوسته و حقیقی از میانگین بحث می‌شود، استاندارد هستند. به عنوان مثال، به Cramer (1946)، Lehman (1991) و Bickel و Doksum (2001) مراجعه کنید. کران‌دار بودن مشتق چهارم یا برخی مشتقات بالاتر برای برقرار بودن گزاره ۳.۱ ضروری است.

شرایط جایگزینی که در آن فرضیات قوی‌تری بر روی توزیع‌های داده \(X_i\) و شرایط ضعیف‌تری روی تابع \(L\) اعمال می‌شود در ادبیات وجود دارد (Khan (2004)). در اینجا \(L\) یک تابع زیان است و به نظر معقول می‌رسد که کران‌دار بودن برخی از مشتقات بالاتر آن را فرض کنیم.

گزاره ۳.۱ یک تقریب از امید ریاضی 
$$(L(\bar{X}_{S_j}, X_i))$$
ارائه می‌دهد.

**گزاره ۳.۱** فرض کنید 
$$(X_1, X_2, \cdots, X_n)$$
متغیرهای تصادفی مستقل و هم‌توزیع باشند به طوری که 
$$(E(X_i) = \mu)، (Var(X_i) = \sigma^2)$$
و گشتاور چهارم متناهی. فرض کنید که \(L\) فرضیات ۱، ۲ و ۳ را ارضا می‌کند. آنگاه:

$$E[L(\bar{X}_{S_j}; X_i)] = E[L(\mu, X_i)]\\ + \frac{\sigma^2}{2n_1} E[(L''(\mu, X_i))]\\ + O(\frac{1}{n_1^2})$$
,که در آن باقیمانده \(R_n\) به گونه‌ای است که 
$$(E(R_n)) (O(\frac{1}{n_1^2}))\$$
است، یعنی وجود دارد $$(n_0) و (A < \infty)$$
به طوری که 
$$(E(R_n) < \frac{A}{n_1^2})، (\forall n > n_0)$$
 و همه 
 $$(\mu)$$
 پریم نشان‌دهنده مشتق نسبت به آرگومان اول \(L\) است.

**اثبات:** از یک استدلال امید ریاضی شرطی استفاده خواهیم کرد. بنویسید:

$$E[L(\bar{X}_{S_j}; X_i)] = E_{S_j, i}{ E_{Z_1^n}[L(\bar{X}_{S_j}; X_i) | S_j, i] }$$
,tag{3.2}

$$(j = 1, 2, \cdots, J)$$
و \(i\) نشان‌دهنده \(X_i\) است و به گونه‌ای است که 
$$(i \in S_j^c)$$

اکنون 
$$(L(\bar{X}_{S_j}; X_i))$$
را نسبت به 
$$(\bar{X}_{S_j})$$
حول میانگین \(\mu\) بسط دهید تا به دست آورید:


$$L(\bar{X}_{S_j}; X_i) = L(\mu, X_i) + L'(\mu, X_i)(\bar{X}_{S_j} - \mu)\\ + \frac{1}{2} L''(\mu, X_i)(\bar{X}_{S_j} - \mu)^2 \quad + \frac{1}{6} L'''(\mu, X_i)(\bar{X}_{S_j} - \mu)^3 \\+ \frac{1}{24} L^{(iv)}(\mu^*, X_i)(\bar{X}_{S_j} - \mu)^4$$
,tag{3.3}

با نشان دادن:

$$R_n = L^{(iv)}(\mu^*, X_i)(\bar{X}_{S_j} - \mu)^4$$

و

$$E_{Z_1^n}\{ R_n | S_j, i \} = E_{Z_1^n}\{ L^{(iv)}(\mu^*, X_i)(\bar{X}_{S_j} - \mu)^4 | S_j, i \}$$
,tag{3.4}

و از آنجایی که توسط فرض ۱ توزیع 
$$(L^{(iv)}(\mu^*, X_i)(\bar{X}_{S_j} - \mu)^4)$$
به تحقق خاص \(S_j\) و \(i\) بستگی ندارد، به دست می‌آوریم:

$$E_{S_j, i}\{ E_{Z_1^n}[L^{(iv)}(\mu^*, X_i)(\bar{X}_{S_j} - \mu)^4 | S_j, i] \}\\ = E[L^{(iv)}(\mu^*, X_i)] E(\bar{X}_{S_j} - \mu)^4 \leq M \cdot E(\bar{X}_{S_j} - \mu)^4$$


این به این دلیل است که توسط فرض ۳ داریم 
$$(E[L^{(iv)}(\mu^*, X_i)] \leq E[M(X_i)] < \infty)$$
اکنون لم A.5 ضمیمه تضمین می‌کند که 
$$(E(\bar{X}_{S_j} - \mu)^4)$$
از مرتبه \(1/n_1^2\) است. بنابراین، با گرفتن امید ریاضی در (3.3) و استفاده از (3.4) به دست می‌آوریم:

$$E[L(\bar{X}_{S_j}; X_i)] = E_{S_j, i}{ E_{Z_1^n}[L(\mu, X_i) | S_j, i] }\\ + E_{S_j, i}{ E_{Z_1^n}[L'(\mu, X_i)(\bar{X}_{S_j} - \mu) | S_j, i] }\\ \quad + E_{S_j, i}\{ E_{Z_1^n}[\frac{1}{2} L''(\mu, X_i)(\bar{X}_{S_j} - \mu)^2 | S_j, i] \}\\ \quad + E_{S_j, i}\{ E_{Z_1^n}[\frac{1}{6} L'''(\mu, X_i)(\bar{X}_{S_j} - \mu)^3 | S_j, i] \}\\ + O(\frac{1}{n_1^2})$$


با فرض ۱، توزیع 
$$(L(\mu, X_i))$$
به تحقق خاص \(S_j\) و \(X_i\) بستگی ندارد. بنابراین:

$$E_{S_j, i}\{ E_{Z_1^n}[L(\mu, X_i) | S_j, i] \} = E_{Z_1^n}[L(\mu, X_i)]$$

استدلال‌های مشابه با موارد فوق، تقریب برای گشتاور اول را به صورت زیر تولید می‌کنند:

$$E[L(\bar{X}_{S_j}; X_i)] = E[L(\mu, X_i)] + \frac{\sigma^2}{2n_1} E[(L''(\mu, X_i))]\\ + O(\frac{1}{n_1^2})$$

**ملاحظه ۱**: توجه کنید که ما فرضیات توزیعی بر روی داده‌ها تحمیل نمی‌کنیم. تنها شرط اعمال شده این است که نمونه‌ها از توزیع‌هایی می‌آیند که گشتاور چهارم برای آنها متناهی است. بسیاری از خانواده‌های استاندارد توزیع‌ها این شرط را ارضا می‌کنند.

**ملاحظه ۲**: نیاز به متناهی بودن گشتاور چهارم برای برقراری گزاره ۳.۱، محدودیت‌هایی بر روی مجموعه‌های داده‌ای که این تخمینگر می‌تواند روی آنها محاسبه شود، تحمیل می‌کند. برای مثال، ممکن است اعمال این روش‌ها بر روی مجموعه‌های داده‌ای که شامل تغییرات بزرگ هستند، مانند آنهایی که از بیمه و امور مالی می‌آیند، نامناسب باشد. از طرف دیگر، نتایج برای برخی توزیع‌های دم‌کلفت، مانند توزیع \(t\) با ۵ یا درجات آزادی بیشتر، اعمال می‌شود. برای مثال، توزیع \(t_5\) یک توزیع دم‌کلفت است که گشتاور چهارم برای آن وجود دارد.

گزاره زیر واریانس زیان 
$$(L(\bar{X}_{S_j}, X_i))$$
را تقریب می‌زند.

**گزاره ۳.۲** اگر فرضیات ۱، ۲ و ۳ برقرار باشند. اگر علاوه بر این، مشتق چهارم 
$$(L^2(\bar{X}_{S_j}, X_i))$$
 کران‌دار باشد، آنگاه:

$$Var[L(\bar{X}_{S_j}; X_i)] = Var[L(\mu, X_i)]\\ + \frac{\sigma^2}{n_1} \left\{ E[(L'(\mu, X_i))^2] + Cov(L(\mu, X_i), L''(\mu, X_i)) \right\}\\ + O(1/n_1^2)$$

که در آن جمله باقیمانده 
$$(O(\frac{1}{n_1^2}))$$
است.

**اثبات:** برای به دست آوردن یک بسط برای واریانس 
$$(L(\bar{X}_{S_j}; X_i))$$
، گزاره ۱ را به تابع 
$$(L^2(\bar{X}_{S_j}; X_i))$$
اعمال کنید با استفاده از این واقعیت که:

$$[L^2(\mu, X_i)]'' = \frac{\partial^2}{\partial \mu^2}[L^2(\mu, X_i)] = 2(L'(\mu, X_i))^2\\ + 2L(\mu, X_i) L''(\mu, X_i)$$
tag{3.5}


سپس با جایگزینی بسط برای 
$$(L(\bar{X}_{S_j}, X_i))$$
و استفاده از فرمول (3.5)، گزاره ۱ و فرمول واریانس شرطی به دست می‌آوریم:

$$Var[L(\bar{X}_{S_j}; X_i)] = Var[L(\mu, X_i)]\\ + \frac{\sigma^2}{n_1} \left\{ E[(L'(\mu, X_i))^2] + Cov(L(\mu, X_i), L''(\mu, X_i)) \right\}\\ + O(1/n_1^2)$$


برای اثبات دو گزاره فوق از یک سری لم استفاده می‌کنیم که نرخ جمله باقیمانده را تضمین می‌کنند. این لم‌ها در ضمیمه ارائه شده‌اند.

اکنون یک مثال نظری ارائه می‌دهیم که تقریب‌های ارائه شده در گزاره‌های ۱ و ۲ را تأیید می‌کند.

**مثال.** فرض کنید 
$$(L(\bar{X}_{S_j}, X_i) = (\bar{X}_{S_j} - X_i)^2)$$
، یعنی زیان مربعات خطا که به طور گسترده استفاده می‌شود. یک محاسبه دقیق از امید ریاضی 
$$((\bar{X}_{S_j} - X_i)^2)$$
 تولید می‌کند:

$$E\{ L(\bar{X}_{S_j}, X_i) \} = Var(\bar{X}_{S_j}) + Var(X_i) = \sigma^2 + \frac{\sigma^2}{n_1}$$

از طرف دیگر، اگر از گزاره ۳.۱ استفاده شود، به دست می‌آوریم:

$$E[L(\bar{X}_{S_j}, X_i)] = E(X_i - \mu)^2\\ + \frac{\sigma^2}{n_1} = \sigma^2 + \frac{\sigma^2}{n_1}$$

و دو فرمول بر هم منطبق هستند. توجه کنید که در مورد زیان مربعات خطا، مشتق دوم زیان، نسبت به \(\mu\)، کران‌دار است. جملات مرتبه 
$$(1/n_1^2)$$
 وارد فرمول نمی‌شوند زیرا همه مشتقات بالاتر از دو برای زیان درجه دوم ۰ هستند. بنابراین، فرمول تقریب با محاسبه دقیق موافقت دارد.

سپس به فرمول واریانس می‌پردازیم. محاسبه دقیق بر اساس فرمول زیر است:

$$Var[L(\bar{X}_{S_j}, X_i)] = E_{S_j, i}{ Var_{Z_1^n}[(\bar{X}_{S_j} - X_i)^2 | S_j, i] }\\ + Var_{S_j, i}{ E_{Z_1^n}[(\bar{X}_{S_j} - X_i)^2 | S_j, i]}$$
,tag{3.6}


با استفاده از این فرمول، واریانس دقیق را به صورت زیر به دست می‌آوریم:

$$Var[L(\bar{X}_{S_j}, X_i)] = 2\sigma^4 + \frac{4\sigma^4}{n_1} + \frac{2\sigma^4}{n_1^2}$$
,tag{3.7}


با استفاده از فرمول داده شده در گزاره ۳.۲، به دست می‌آوریم که واریانس تقریبی است:

$$Var[L(j, i)] = 2\sigma^4 + \frac{4\sigma^4}{n_1} + O(\frac{1}{n_1^2})$$
,tag{3.8}


با مقایسه این دو فرمول، می‌بینیم که فرمول تقریب واریانس تمام جملات مرتبه اول را شناسایی می‌کند.

گزاره زیر فرمول تقریب را برای جملات کوواریانس که در محاسبه واریانس تخمین‌گرهای اعتبارسنجی متقابل خطای تعمیم وارد می‌شوند، estable می‌کند.

**گزاره ۳.۳** فرض کنید 
$$(S_j)، (S_{j'})$$
دو مجموعه آموزش باشند که به طور مستقل و تصادفی از جهان داده 
$$(Z_1^n)$$
کشیده شده‌اند، و 
$$(S_j^c)، (S_{j'}^c)$$
مجموعه‌های آزمون متناظر باشند. اجازه دهید 
$$(X_i \in S_j^c, X_{i'} \in S_{j'}^c)، (D = S_j \cap S_{j'})$$
,
$$(Y = Card(D))$$
. سپس، اگر 
$$(i \neq i')$$
:

$$Cov[L(\bar{X}_{S_j}, X_i), L(\bar{X}_{S_{j'}}, X_{i'})]\\ = \frac{\sigma^2}{n_1^2} E(Y) (E[L'(\mu, X_i)])^2\\ - \frac{\sigma^4}{4n_1^2} (E[L''(\mu, X_i)])^2 + O(\frac{1}{n_1^2})$$

اگر (i = i'):

$$Cov[L(\bar{X}_{S_j}, X_i), L(\bar{X}_{S_{j'}}, X_{i'})]\\ = Var(L(\mu, X_i)) \\
+ \frac{\sigma^2}{n_1} \left\{ E[L(\mu, X_i) L''(\mu, X_i)] - E[L(\mu, X_i)] E[L''(\mu, X_i)] \right\}\\ + \frac{\sigma^2}{n_1^2} E(Y) E[L'(\mu, X_i)]^2 - \frac{\sigma^4}{4n_1^2} \{ E[L''(\mu, X_i)] \}^2 + O(\frac{1}{n_1^2})$$


که در آن \(E(Y)\) امید ریاضی متغیر تصادفی \(Y\) با respect به توزیع آن است.

این گزاره نشان می‌دهد که تغییرپذیری ناشی از نمونه‌گیری تصادفی مجموعه‌های آموزش \(S_j\) توسط امید ریاضی متغیر تصادفی 
$$(Y = Card(S_j \cap S_{j'}))، (j \neq j')، (j, j' \in 1, 2, \cdots, J)$$
، کمی می‌شود. از آنجایی که 
$$(S_j)، (S_{j'})$$
مجموعه‌های تصادفی با \(n_1\) عنصر هستند، \(Y\) به گونه‌ای است که 
$$(max(0, 2n_1 - n) \leq Y \leq n_1)$$

یک متغیر تصادفی اضافی که وارد تخمینگر واریانس تخمینگر اعتبارسنجی متقابل خطای تعمیم می‌شود، 
$$(Y^* = Card(S_j^c \cap S_{j'}^c))$$
است، اندازه اشتراک دو مجموعه آزمون مختلف. دو لم زیر توزیع این دو متغیر تصادفی را به دست می‌آورند.

**لم ۳.۱** فرض کنید 
$$(S_j) ,(S_{j'})$$
مجموعه‌های تصادفی با \(n_1\) عنصر متمایز از \(Z_1^n\) باشند فرض کنید 
$$(Y = Card(S_j \cap S_{j'}))، (\max(0, 2n_1 - n) \leq Y \leq n_1)$$
سپس، توزیع \(Y\) است:

$$P(Y = y) = \frac{\binom{n_1}{y} \binom{n - n_1}{n_1 - y}}{\binom{n}{n_1}}$$

یک توزیع فوق‌هندسی.

**اثبات.** ما مسئله را به عنوان جدول 
$$(2 \times n)$$
 زیر مدل می‌کنیم.

| k | 1 | 2 | 3 | \(\cdots\) | n | Total |
|---|---|---|---|---|---|---|
| \(S_j\) | 0 | 1 | 1 | \(\cdots\) | 0 | \(n_1\) |
| \(S_{j'}\) | 1 | 0 | 1 | \(\cdots\) | 0 | \(n_1\) |
|   | \(a_1\) | \(a_2\) | \(a_3\) | \(\cdots\) | \(a_n\) | \(2n_1\) |

در جدول نشان می‌دهیم که آیا مولفه \(k\)ام 
$$(Z_1^n)$$
در مجموعه آموزش \(S_j\) یا \(S_{j'}\) نمونه‌گیری شده است با ۱، در غیر این صورت با ۰ نشان می‌دهیم. با \(a_k\) مجموع نشانگرها برای مولفه \(k\)ام در جامعه 
$$(Z_1^n)$$
روی \(S_j\) و \(S_{j'}\) را نشان دهید. سپس:

$$a_1 + a_2 + \cdots + a_n = 2n_1 \\
0 \leq a_i \leq 2$$
$$i = 1, \cdots, n$$

اکنون، \(P(Y = y)\) معادل 
$$(P(\#\{a_i = 2\}))$$
، 
$$(i = 1, \cdots, n)$$
است. با توجه به \(Y = y\)، تعداد \(\{a_i = 1\}\) \(2n_1 - 2y\) و تعداد \(\{a_i = 0\}\) \(n - 2n_1 + y\) است. از آنجایی که هیچ یک از این سه عدد نمی‌تواند منفی باشد، دامنه Y را به صورت 
$$(max(0, 2n_1 - n) \leq Y \leq n_1)$$
به دست می‌آوریم. همچنین به یاد داشته باشید که \(S_j\)، \(S_{j'}\) به طور مستقل نمونه‌گیری شده‌اند و هر کدام شامل \(n_1\) عنصر هستند. با توجه به \(Y = y\)، توزیع مجموع ستون‌ها ثابت است. یعنی \(a_i\) فقط می‌تواند مقادیر ۰، ۱ یا ۲ را بگیرد. تعداد جداول مختلف با همان مجموع ستون‌ها سپس 
$$(\binom{n}{y} \binom{n - y}{n_1 - y} \binom{n - n_1}{n_1 - y})$$
 است. و بنابراین:

$$P(Y = y) = \frac{\binom{n}{y} \binom{n - y}{n_1 - y} \binom{n - n_1}{n_1 - y}}{\binom{n}{n_1} \binom{n}{n_1}} = \frac{\binom{n_1}{y} \binom{n - n_1}{n_1 - y}}{\binom{n}{n_1}}$$

توزیع فوق‌هندسی.

**لم ۳.۲** فرض کنید 
$$(S_j) ,(S_{j'})$$
دو مجموعه آموزش باشند و 
$$(S^c_j) ,(S^c_{j'})$$
مجموعه‌های آزمون متناظر آنها باشند.فرض کنید  
$$(Y^* = Card(S^c_j \cap S^c_{j'}))، (0 \leq Y^* \leq n - n_1)$$
. سپس:

$$P(Y^* = y) = \frac{\binom{n_1}{y - n + 2n_1} \binom{n - n_1}{n - n_1 - y}}{\binom{n}{n_1}}\\ = \frac{\binom{n_2}{n_2 - y} \binom{n - n_2}{n - n_2 - (n_2 - y)}}{\binom{n}{n - n_2}}$$


**اثبات.** از اثبات لم ۳.۱ 
$$(P(Y^* = y) = P(\#\{a_i = 0\}))$$
، 
$$(\{i = 1, \cdots, n\})$$
علاوه بر این، 
$$(Y^* = n - 2n_1 + Y)$$
 سپس، نتیجه به دست می‌آید.

قضیه ۳.۱ تخمینگر واریانس 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
را ارائه می‌دهد. ابتدا قضیه را بیان می‌کنیم.

**قضیه ۳.۱.** واریانس تخمینگر خطای تعمیم 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
به صورت زیر داده می‌شود:

$$Var({}^{n_2}_{n_1}\hat{\mu}_J) = \frac{1}{J} Var(\hat{\mu}_j) + \frac{J - 1}{J} Cov(\hat{\mu}_j, \hat{\mu}_{j'})$$

که در آن:

$$Var(\hat{\mu}_j) = \frac{1}{n_2} \left[ Var[L(\mu, X_i)] + \frac{\sigma^2}{n_1} \{ E[(L'(\mu, X_i))^2] + Cov(L(\mu, X_i), L''(\mu, X_i)) \} \right] \\
 + \frac{n_2 - 1}{n_2} \frac{\sigma^2}{n_1} \{ E[L'(\mu, X_i)] \}^2 + O(1/n_1^2),$$

$$Cov(\hat{\mu}_j, \hat{\mu}_{j'})\\ = (1 - \frac{E(Y^*)}{n_2^2}) \left[ \frac{\sigma^2}{n_1^2} E(Y) (E[L'(\mu, X_i)])^2 - \frac{\sigma^4}{4n_1^2} (E[L''(\mu, X_i)])^2 + O(\frac{1}{n_1^2}) \right] \\
 + \frac{E(Y^*)}{n_2^2} \left[ Var(L(\mu, X_i)) + \frac{\sigma^2}{n_1} \{ E[L(\mu, X_i) L''(\mu, X_i)] - E[L(\mu, X_i)] E[L''(\mu, X_i)] \} \right. \\ \left. + \frac{\sigma^2}{n_1^2} E(Y) E[L'(\mu, X_i)]^2 - \frac{\sigma^4}{4n_1^2} \{ E[L''(\mu, X_i)] \}^2 + O(\frac{1}{n_1^2}) \right]$$

که در آن 
$$(\mu = E_{Z_1^n} X_i)، (\sigma^2 = Var_{Z_1^n}(X_i))$$

فرمول‌های فوق به وضوح وابستگی 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
را به گشتاور اول متغیرهای تصادفی 
$$(Y)، (Y^*)$$
نشان می‌دهند. از آنجایی که توزیع 
$$(Y)، (Y^*)$$
شناخته شده است، می‌توانیم 
$$(E(Y))، (E(Y^*))$$
را با مقادیر متناظرشان جایگزین کرده و عبارات فوق را ساده کنیم. از آنجایی که توزیع 
$$(Y)، (Y^*)$$
  فوق‌هندسی است 
$$(E(Y) = \frac{n_1^2}{n}) و (E(Y^*) = \frac{n_2^2}{n})$$
  سپس:

$$Cov(\hat{\mu}_j, \hat{\mu}_{j'})\\ = (1 - \frac{1}{n}) \left[ \frac{\sigma^2}{n} (E[L'(\mu, X_i)])^2 - \frac{\sigma^4}{4n_1^2} (E[L''(\mu, X_i)])^2 + O(\frac{1}{n_1^2}) \right] \\
 + \frac{1}{n} \left[ Var(L(\mu, X_i)) + \frac{\sigma^2}{n_1} \{ Cov(L(\mu, X_i), L''(\mu, X_i)) \} \right. \\
\left. + \frac{\sigma^2}{n} E[L'(\mu, X_i)]^2 - \frac{\sigma^4}{4n_1^2} (E[L''(\mu, X_i)])^2 + O(\frac{1}{n_1^2}) \right]$$

تخمینگر نهایی واریانس 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
یک تخمینگر plug-in است و می‌تواند با استفاده از قضیه (3.1) محاسبه شود. ما نیاز داریم که میانگین جامعه ناشناخته \(\mu\) و واریانس جامعه \(\sigma^2\) را به ترتیب توسط تخمینگرهایشان، میانگین نمونه و واریانس نمونه جایگزین کنیم. اگر محاسبه واریانس نمونه و میانگین بر اساس جهان داده مناسب نباشد، می‌توانیم 
$$(\bar{X}_{S_j})$$
را محاسبه کنیم و، اگر مجموعه‌های آموزش مختلف زیادی وجود داشته باشند، به عنوان تخمینگر میانگین نمونه 
$$(\bar{X} = \frac{1}{J} \sum_{j=1}^{J} \bar{X}_{S_j})$$
را در نظر بگیریم. علاوه بر این، 
$$(\hat{\sigma}_j^2 = \frac{1}{n_1 - 1} \sum_{l=1}^{n_1} (X_l - \bar{X}_{S_j})^2)$$
، بنابراین تخمین واریانس برای واریانس جامعه خواهد بود 
$$(\hat{\sigma}^2 = \frac{1}{J} \sum_{j=1}^{J} \hat{\sigma}_j^2)$$

**مثال.** در مورد زیان مربعات خطا، تقریب‌های برای واریانس 
$$(\hat{\mu}_j) و (Cov(\hat{\mu}_j, \hat{\mu}_{j'}))$$
به صورت زیر داده می‌شوند:

$$Var(\hat{\mu}_j) = \frac{1}{n_2} Var[(X_i - \mu)^2] + \frac{4\sigma^4}{n_1 n_2}\\ = \frac{1}{n_2} E[(X_i - \mu)^4] - \frac{\sigma^4}{n_2} + \frac{4\sigma^4}{n_1 n_2}$$
,tag{3.9}
$$Cov(\hat{\mu}_j, \hat{\mu}_{j'}) = (1 - \frac{1}{n}) (-\frac{\sigma^4}{n_1^2})\\ + \frac{1}{n} (\frac{4\sigma^4}{n} - \frac{\sigma^4}{n_1^2} + Var[(X_i - \mu)^2])$$
,tag(3.10)


اگر داده‌ها از 
$$(N(0, \sigma^2))$$
باشند، آنگاه تخمینگر تقریب گشتاور برای واریانس 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
توسط زیر داده می‌شود:

$$\hat{\sigma}^4 \left\{ \frac{2(n_1 + 2)}{n_1 n_2} \frac{1}{J} + (\frac{J - 1}{J}) \left[ \frac{2(n + 2)}{n^2} - \frac{1}{n_1^2} \right] \right\}$$

که در آن 
$$(\hat{\sigma})$$
انحراف معیار نمونه است. بنابراین تخمینگر واریانس 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
مضربی از واریانس نمونه است و فاکتور ضرب وابستگی تخمینگر به 
$$(n_1, n_2) و (n)$$
را نشان می‌دهد.

**تخمینگر واریانس تخمینگر اعتبارسنجی متقابل k-دسته‌ای خطای تعمیم.**

در اینجا یک تخمینگر واریانس برای تخمینگر اعتبارسنجی متقابل k-دسته‌ای خطای تعمیم یک الگوریتم یادگیری ارائه می‌دهیم. توجه کنید که این یک مورد خاص از قضیه ۳.۱ است. در اعتبارسنجی متقابل \(k\)-دسته‌ای، جهان داده به \(k\) مجموعه آزمون غیرهمپوشان مختلف تقسیم می‌شود، که هر کدام شامل 
$$(\frac{n}{k})$$
عنصر هستند. تعداد عناصر \(n_1\)، در هر مجموعه آموزش داده شده، سپس 
$$(n - \frac{n}{k} = \frac{(k - 1)n}{k})$$
است. بنابراین، 
$$(Y = Card(S_j \cap S_{j'}) = \frac{(k - 2)n}{k})$$
قضیه ۳.۱ تقریب‌های زیر را می‌دهد:

$$Var(\hat{\mu}_j)\\ = \frac{k}{n} \left[ Var(L(\mu, X_i)) + \frac{\sigma^2}{n} (\frac{k}{k - 1}) \{ E[(L'(\mu, X_i))^2] + Cov(L(\mu, X_i), L''(\mu, X_i)) \} \right] \\
 + \frac{n - k}{n} \frac{\sigma^2}{n} \frac{k}{k - 1} \{ E[L'(\mu, X_i)] \}^2 + O(1/n_1^2)$$

و

$$Cov(\hat{\mu}_j, \hat{\mu}_{j'}) = \frac{\sigma^2}{n} \frac{k(k - 2)}{(k - 1)^2} (E[L'(\mu, X_i)])^2 - \frac{\sigma^4}{4n^2} (\frac{k}{k - 1})^2 (E[L''(\mu, X_i)])^2 + O(\frac{1}{n_1^2})$$

بنابراین، تخمین واریانس را می‌توان با استفاده از رابطه (3.1) محاسبه کرد، که در آن 
$$(Var(\hat{\mu}_j)) و (Cov(\hat{\mu}_j, \hat{\mu}_{j'}))$$
با تخمین‌هایشان جایگزین شده‌اند. اینها را می‌توان با جایگزینی 
$$(\mu)، (\sigma^2)$$
توسط تخمین‌های نمونه‌ای آنها با استفاده از داده‌های مجموعه‌های آموزش به دست آورد.

اکنون فرض کنید که تابع زیان مورد استفاده مربعات خطا است. در این حالت، 
$$(L'(\mu, x_i) = 2(\mu - x_i)) و (L''(\mu, x_i) = 2)$$
. فرمول‌ها سپس برای واریانس 
$$(\hat{\mu}_j)$$
و کوواریانس بین 
$$(\hat{\mu}_j)$$
های مختلف به صورت زیر ساده می‌شوند:

$$Var(\hat{\mu}_j) = \frac{k}{n} \left\{ Var[(X_i - \mu)^2] + \frac{4\sigma^4}{n} (\frac{k}{k - 1}) \right\}$$
,tag{3.11}
$$Cov(\hat{\mu}_j, \hat{\mu}_{j'}) = -\frac{\sigma^4}{n^2} (\frac{k}{k - 1})^2, \quad j \neq j'$$
,tag(3.12)

سپس 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
را می‌توان با استفاده از فرمول (3.1) و جایگزینی 
$$(\sigma^2) و (Var[(X_i - \mu)^2])$$
توسط واریانس نمونه و یک تخمین نمونه مناسب برای 
$$(Var[(X_i - \mu)^2])$$
تخمین زد. تقریب نهایی واریانس 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
سپس به صورت زیر است:

$$Var({}^{n_2}_{n_1}\hat{\mu}_J) = \frac{1}{n} \{ Var[(X_i - \mu)^2] \}\\ + \frac{3k\sigma^4}{(k - 1)n^2} = \frac{1}{n} E[(X_i - \mu)^4]\\ - \frac{\sigma^4}{n} + \frac{3k\sigma^4}{(k - 1)n^2}$$

یک تخمینگر ساده برای 
$$(E[(X_i - \mu)^4])$$
را می‌توان از نمونه آموزش با گرفتن نسخه نمونه‌ای از امید ریاضی فوق، 
$$(\frac{1}{n_1} \sum_{i \in S_j} (X_i - \bar{X}_{S_j})^4)$$
محاسبه کرد. برای نشان دادن، اگر بیشتر یک جامعه نرمال را فرض کنیم، آنگاه 
$$(Var[(X_i - \mu)^2] = 2\sigma^4)$$
و تخمینگر واریانس 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
به صورت زیر داده می‌شود:

$$\frac{\hat{\sigma}^4}{n} (2 + \frac{3k}{n(k - 1)})$$

که در آن \(\hat{\sigma}\) انحراف معیار نمونه است.

## ۳.۲. مورد رگرسیون
مورد رگرسیون مورد دیگری از برازش میانگین‌ها است. ما در اینجا مسئله تخمین واریانس تخمینگر اعتبارسنجی متقابل خطای تعمیم 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
را در مورد رگرسیون در نظر می‌گیریم. بنابراین داده‌ها تحقق‌هایی از متغیرهای تصادفی 
$$((Y_i, X_i))،(i = 1, 2, \cdots, n)$$
هستند به طوری که 
$$(E(Y_i | X_i) = x_i^T \beta)$$
توجه کنید که متغیرهای توضیحی در اینجا ثابت در نظر گرفته شده‌اند. این فرمول‌بندی به عنوان مورد طرح ثابت شناخته می‌شود. بردار پارامترهای ناشناخته 
$$(\beta)$$
معمولاً توسط حداقل مربعات تخمین زده می‌شود. با 
$$(\hat{\beta})$$
تخمینگر حداقل مربعات 
$$(\beta)$$
را نشان دهید. سپس برای یک مشاهده جدید 
$$((y_i, x_i) \in S_j^c) با (\hat{y}_{i, S_j} = x_i^T \hat{\beta}_{S_j})$$
نشان دهید، که در آن 
$$(\hat{\beta}_{S_j})$$
نشان‌دهنده تخمینگر 
$$(\beta)$$
است که با استفاده از داده‌های مجموعه آموزش 
$$(S_j)$$
محاسبه شده است. تابع زیان \(L\) سپس به 
$$(\hat{y}_{i, S_j})$$
و \(y_i\) وابسته است، یعنی 
$$(L(\hat{y}_{i, S_j}, y_i))$$

برای استخراج تخمینگر 
$$(Var({}^{n_2}_{n_1}\hat{\mu}_J))$$
نیاز داریم از روش تقریب گشتاور برای به دست آوردن تقریب‌هایی برای گشتاورهای آماره 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
استفاده کنیم. ایده همانند مورد تخمین میانگین ساده است. یعنی، تابع زیان با respect به آرگومان اولش بسط داده می‌شود و در نقطه 
$$(E(Y_i | X_i) = x_i^T \beta_0)$$
، که در آن 
$$(\beta_0)$$
مقدار پارامتر واقعی است، ارزیابی می‌شود. به عبارت دیگر، مانند قبل، بسط در میانگین واقعی ارزیابی می‌شود.

اکنون فرضیاتی که تحت آن نظریه ما برقرار است را فهرست می‌کنیم.

**فرض ۱.** اگر \(S_j\) یک مجموعه آموزش با \(n_1\) تعداد عناصر باشد:

$$\lim_{n_1 \rightarrow \infty} \frac{1}{n_1} (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} = V$$

که در آن \(V\) متناهی و معین مثبت است.

**فرض ۲.** فرض کنید 
$$(x_{n_1 k})$$
نشان‌دهنده سطر \(k\)ام ماتریس طرح 
$$(\mathbf{X}_{S_j})$$
باشد. سپس، برای هر 
$$(j = 1, 2, \cdots, J)$$

$$\max_{1 \leq k \leq n_1} x_{n_1 k} (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} x_{n_1 k} \rightarrow 0$$

وقتی 
$$(n_1 \rightarrow \infty)$$

توجه کنید که این شرط به عنوان شرط نواتر تعمیم‌یافته شناخته می‌شود.

تحت شرایط فوق 
$$(\sqrt{n_1} (\hat{\beta}_{S_j} - \beta))$$
در توزیع به یک متغیر تصادفی 
$$(N(0, \sigma^2 V))$$
همگرا می‌شود.

گزاره زیر یک تقریب برای امید ریاضی تابع زیان \(L\) estable می‌کند.

**گزاره ۳.۴**: فرض کنید که فرضیات ۱ و ۲ برقرار باشند. سپس:

$$E[L(\hat{y}_{i, S_j}, y_i)] = E[L(x_i^T \beta_0, y_i)]\\ + \frac{\sigma^2}{2} E[L''(x_i^T \beta_0, y_i)] \text{tr}[(x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1}]\\ + R_n$$

که در آن جمله باقیمانده از مرتبه 
$$(O(\frac{1}{n_1^2}))$$
است، و پریم نشان‌دهنده مشتق نسبت به آرگومان اول تابع زیان است.

**اثبات:** ابتدا 
$$(L(\hat{y}_{i, S_j}, y_i))$$
را با respect به آرگومان اول بسط دهید تا به دست آورید:

$$L(\hat{y}_{i, S_j}, y_i) = L(x_i^T \beta_0, y_i)\\ + L'(x_i^T \beta_0, y_i) x_i^T (\hat{\beta}_{S_j} - \beta_0) \\
 + \frac{1}{2} L''(x_i^T \beta_0, y_i) (\hat{\beta}_{S_j} - \beta_0)^T x_i x_i^T (\hat{\beta}_{S_j} - \beta_0)\\ + R_n$$
,tag{3.13}

که در آن \(R_n\) نشان‌دهنده جمله باقیمانده است.

اکنون:

$$E\{ L(\hat{y}_{i, S_j}, y_i) \} = E_{S_j, i}\{ E_{Z_1^n}[L(\hat{y}_{i, S_j}, y_i) | S_j, i] \} \\
= E_{S_j, i}\{ E_{Z_1^n}[L(x_i^T \beta_0, y_i) | S_j, i] \}\\ + E_{S_j, i}\{ E_{Z_1^n}[L'(x_i^T \beta_0, y_i) x_i^T | S_j, i] E_{Z_1^n}[(\hat{\beta}_{S_j} - \beta_0) | S_j, i] \} \\
 + \frac{1}{2} E_{S_j, i}\{ E_{Z_1^n}[L''(x_i^T \beta_0, y_i) | S_j, i] E_{Z_1^n}[(\hat{\beta}_{S_j} - \beta_0)^T x_i x_i^T (\hat{\beta}_{S_j} - \beta_0) | S_j, i] \}$$

اما امید ریاضی 
$$(E_{Z_1^n}[(\hat{\beta}_{S_j} - \beta_0) | S_j, i] = 0)$$
زیرا 
$$(E_{Z_1^n}(\hat{\beta}_{S_j} | S_j, i) = E_{Z_1^n}(\hat{\beta}_{S_j}) = \beta_0)$$
همچنین از آنجایی که توزیع 
$$(\hat{\beta}_{S_j})$$
به طور مجانبی 
$$(N(\beta_0, \sigma^2 (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1}))$$
است، تحت فرضیات ۱ و ۲ به دست می‌آوریم:

$$E_{S_j, i}\{ E_{Z_1^n}[(\hat{\beta}_{S_j} - \beta_0)^T x_i x_i^T (\hat{\beta}_{S_j} - \beta_0) | S_j, i] \} = E_{Z_1^n}[(\hat{\beta}_{S_j} - \beta_0)^T x_i x_i^T (\hat{\beta}_{S_j} - \beta_0)] \\
= \sigma^2 \text{tr}[(x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1}]$$


که در آن 
$$(\sigma^2 = Var_{Z_1^n}(X_i))$$
واریانس نمونه، و 
$$(\text{tr}(A))$$
نشان‌دهنده اثر ماتریس \(A\) است. بنابراین:

$$E[L(\hat{y}_{i, S_j}, y_i)] = E[L(x_i^T \beta_0, y_i)]\\ + \frac{\sigma^2}{2} E[L''(x_i^T \beta_0, y_i)] \text{tr}[(x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1}]\\ + R_n$$

که در آن امیدهای ریاضی با respect به توزیع داده گرفته شده‌اند. علاوه بر این، \(R_n\) از مرتبه 
$$(\frac{1}{n_1^2})$$
است.

گزاره ۳.۵ تقریب برای واریانس 
$$(L(\hat{y}_{i, S_j}, y_i))$$
را estable می‌کند.

**گزاره ۳.۵** فرض کنید که فرضیات ۱ و ۲ برقرار باشند. سپس 
$$(Var(L(\hat{y}_{i, S_j}, y_i)))$$
را می‌توان به صورت زیر تقریب زد:

$$Var\{ L(\hat{y}_{i, S_j}, y_i) \} = Var[L(x_i^T \beta_0, y_i)]\\ + \sigma^2 \text{tr}[(x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} \{ Cov(L(x_i^T \beta_0, y_i), \\
 L''(x_i^T \beta_0, y_i)) + E[L'(x_i^T \beta_0, y_i)]^2 \} + R_n]$$

که در آن 
$$(\sigma^2 = Var_{Z_1^n}(Y_i | X_i)) و (R_n)$$
جمله باقیمانده از مرتبه 
$$(\frac{1}{n_1^2})$$
است.

**اثبات:** اثبات مشابه با گزاره ۳.۲ است، به این صورت که گزاره ۳.۴ را به 
$$(L^2(\hat{y}_{i, S_j}, y_i))$$
اعمال می‌کنیم و از این واقعیت استفاده می‌کنیم که:

$$[L^2(\hat{y}_{i, S_j}, y_i)]'' = 2 L(\hat{y}_{i, S_j}, y_i) L''(\hat{y}_{i, S_j}, y_i)\\ + 2 [L'(\hat{y}_{i, S_j}, y_i)]^2$$

که در آن پریم نشان‌دهنده مشتق نسبت به آرگومان اول تابع زیان است.

**مثال.** برای تأیید تقریب‌های فوق از 
$$(L(\hat{y}_{i, S_j}, y_i) = (\hat{y}_{i, S_j} - y_i)^2)$$
، زیان مربعات خطا و مورد رگرسیون ساده استفاده می‌کنیم، یعنی:

$$y_i = a + b z_i + \epsilon_i = x_i^T \beta + \epsilon_i$$


که در آن 
$$(x_i^T = (1, z_i))، (\beta^T = (a, b)) و ((y_i, x_i) \in S_j^c)$$
نماد 
$$(\hat{y}_{i, S_j})$$
نشان‌دهنده 
$$(x_i^T \hat{\beta}_{S_j})$$
است. امید ریاضی دقیق 
$$(L(\hat{y}_{i, S_j}, y_i) = (x_i^T \hat{\beta}_{S_j} - y_i)^2)$$
به صورت زیر داده می‌شود:

$$E[L(\hat{y}_{i, S_j}, y_i)] = \sigma^2 + \sigma^2 x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} x_i$$

امید ریاضی تقریبی است:

$$E[L(\hat{y}_{i, S_j}, y_i)] = \sigma^2 + \sigma^2 \text{tr}(x_i x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1})$$

از آنجایی که 
$$(\text{tr}(x_i x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1}) = x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} x_i)$$
، تقریب برای امید ریاضی با محاسبه دقیق موافقت دارد. به طور مشابه می‌توانیم تأیید کنیم که تقریب واریانس همان نتیجه محاسبه دقیق را تولید می‌کند. برای نشان دادن بیشتر فرمول‌ها فرض کنید 
$$(y_i \sim N(x_i^T \beta, \sigma^2))$$
، سپس محاسبه دقیق واریانس 
$$(L(\hat{y}_{i, S_j}, y_i))$$
را می‌دهد،

$$Var(L(\hat{y}_{i, S_j}, y_i)) = 2\sigma^4 + 4\sigma^4 x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} x_i + 2\sigma^4 (x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} x_i)^2$$

تقریب توسط زیر داده می‌شود:

$$Var(L(\hat{y}_{i, S_j}, y_i)) = 2\sigma^4 + 4\sigma^4 x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} x_i + O(\frac{1}{n_1^2})$$

یعنی آنها تا جملات مرتبه اول موافقت دارند.

برای کامل کردن تقریب واریانس تخمینگر 
$$({}^{n_2}_{n_1}\hat{\mu}_J)$$
نیاز به یک تقریب از کوواریانس بین 
$$(L(\hat{y}_{i, S_j}, y_i)) و (L(\hat{y}_{i', S_{j'}}, y_{i'}))$$
داریم. گزاره زیر تقریب 
$$(Cov(L(\hat{y}_{i, S_j}, y_i), L(\hat{y}_{i', S_{j'}}, y_{i'})))$$
را بیان می‌کند.

**گزاره ۳.۶.** فرض کنید که فرضیات ۱ و ۲ برقرار باشند. سپس برای 
$$(j \neq j')، (j, j' \in \{1, 2, \cdots, J\}) وقتی (i \neq i')$$
:

$$Cov(L(\hat{y}_{i, S_j}, y_i), L(\hat{y}_{i', S_{j'}}, y_{i'}))\\ = \sigma^2 (E[L'(x_i^T \beta_0, y_i)])^2 x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_{j'}})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} x_{i'} \\
\quad + \frac{\sigma^4}{2} (E[L''(x_i^T \beta_0, y_i)])^2 \text{tr}((x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} (x_{i'} x_{i'}^T) \\
\quad (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1})$$


وقتی \(i = i'\):

$$Cov(L(\hat{y}_{i, S_j}, y_i), L(\hat{y}_{i', S_{j'}}, y_{i'}))\\ = Var(L(x_i^T \beta_0, y_i)) + \frac{\sigma^2}{2} Cov(L(x_i^T \beta_0, y_i), L''(x_i^T \beta_0, y_i)) \\
\quad (x_i^T (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} x_i + x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} x_i) \\
\quad + \sigma^2 (E[L'(x_i^T \beta_0, y_i)])^2 x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_{j'}})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} x_i \\
\quad + \frac{\sigma^4}{2} (E[L''(x_i^T \beta_0, y_i)])^2 \text{tr}((x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_{j'}})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} (x_i x_i^T) \\
\quad (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1}) \\
\quad + \frac{\sigma^4}{4} Var(L''(x_i^T \beta_0, y_i)) x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} x_i x_i^T (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} x_i$$


**گزاره ۳.۷.** فرض کنید \(S_j\) یک مجموعه آموزش باشد، 
$$(j = 1, 2, \cdots, J)$$
 سپس برای 
$$(i \neq i')$$
:

$$Cov(L(\hat{y}_{i, S_j}, y_i), L(\hat{y}_{i', S_j}, y_{i'}))\\ = \sigma^2 (E[L'(x_i^T \beta_0, y_i)])^2 \text{tr}[(x_{i'} x_{i'}^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1}] \\
\quad + \frac{\sigma^4}{2} (E[L''(x_i^T \beta_0, y_i)])^2 \text{tr}[(x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} (x_{i'} x_{i'}^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1}]$$

اثبات‌های گزاره ۳.۶ و گزاره ۳.۷ را می‌توان در ضمیمه C یافت.

**ملاحظه:** اگر زیان مربعات خطا باشد،

$$Cov(L(\hat{y}_{i, S_j}, y_i), L(\hat{y}_{i', S_j}, y_{i'})) = 2\sigma^4 \text{tr}[(x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} (x_{i'} x_{i'}^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1}]$$
,tag{3.14}


برای تخمین رابطه (3.14) فقط نیاز به تخمین \(\sigma\) داریم. ما \(\sigma\) را توسط میانگین مربعات خطای باقیمانده تخمین می‌زنیم.

تحت زیان مربعات خطا، داریم:

$$Var(\hat{\mu}_j) = \frac{1}{n_2^2} \sum_{i=1}^{n_2} \{ 2\sigma^4 + 4\sigma^4 x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} x_i \} + \frac{1}{n_2^2} \sum_{i \neq i'} 2\sigma^4 (x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} x_{i'})^2$$
,tag{3.15} 

$$Cov(\hat{\mu}_j, \hat{\mu}_{j'}) = \frac{1}{n_2^2} \sum_{i \in S_j^c} \sum_{i' \in S_{j'}^c, i \neq i'} \{ 2\sigma^4 \text{tr}\{ (x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} (x_{i'} x_{i'}^T) \\
 (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} \} \} \\
 + \frac{1}{n_2^2} \sum_{i \in S_j^c} \sum_{i' \in S_{j'}^c, i = i'} \{ 2\sigma^4 + 4\sigma^4 x_i^T (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} x_i \\
 + 2\sigma^4 \text{tr}\{ (x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} (x_i x_i^T) \\
 (\mathbf{X}_{S_{j'}}^T \mathbf{X}_{S_{j'}})^{-1} (\mathbf{X}_1^T \mathbf{X}_1) (\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1} \} \}$$
,tag{3.16}


تخمین نهایی از رابطه (3.1) به دست می‌آید که در آن 
$$(Var(\hat{\mu}_j))$$
با استفاده از رابطه (3.15) تخمین زده می‌شود، 
$$(Cov(\hat{\mu}_j, \hat{\mu}_{j'}))$$
با استفاده از رابطه (3.16) تخمین زده می‌شود و \(\sigma^2\) توسط یک تخمینگر از آن جایگزین می‌شود. برای به دست آوردن یک تخمینگر برای \(\sigma^2\)، مدل رگرسیون را برازش می‌کنیم و 
$$(\hat{y}_i)$$
را به دست می‌آوریم. سپس 
$$(\hat{\sigma}^2)$$
واریانس نمونه خطاهای 
$$(\hat{\varepsilon}_i = y_i - \hat{y}_i)$$
است، یعنی میانگین مربعات خطای باقیمانده.

**ملاحظه:** توجه کنید که برای استخراج نتایج فوق، ما از توزیع شرطی \(Y\) \(X\) به عنوان توزیع داده استفاده کردیم، در واقع \(X\) را ثابت در نظر گرفتیم. اکنون، فرض کنید که به جای استفاده از توزیع شرطی به عنوان توزیع داده، \(X\) را تصادفی در نظر بگیریم و از توزیع مشترک \((X, Y)\) استفاده کنیم. در این مورد، توزیع داده است:

$$f(x, y) = g(y - x^T \beta | x) k(x)$$

که در آن 
$$(g(\cdot))$$
توزیع خطاها و 
$$(k(\cdot))$$
توزیع \(x\)ها است. ما سپس می‌توانیم فرمول‌های بیان‌کننده امید ریاضی، واریانس و جملات کوواریانس مورد نیاز را با استفاده از توزیع مشترک \((X, Y)\) استخراج کنیم. برای مثال،
$$(E(\hat{\beta}) = E_{(X, Y)}[(X^T X)^{-1} X^T Y] = E_X\{ E_{Y|X}[(X^T X)^{-1} X^T Y | X] \} = \beta_0)$$
هنوز بی‌طرف است، و 
$$(Var(\hat{\beta}) = E_X\{ Var_Y(\hat{\beta} | X) \} + Var_X\{ E_Y(\hat{\beta} | X) \} = \sigma^2 E_X[(X^T X)^{-1}])$$
تعدیلات دیگری که توزیع \(X\) را در نظر می‌گیرند مورد نیاز هستند. اینها عمدتاً بر گرفتن امیدهای ریاضی، روی \(X\)، از جمله‌هایی که توابعی از \(X\)ها هستند، متمرکز می‌شوند و می‌توانند به راحتی از داده‌ها با استفاده از بوت‌استرپ محاسبه شوند. به عنوان نشانه، تحت زیان مربعات خطا، فرمول در گزاره ۳.۴ می‌شود 
$$(E[L(\hat{y}_{i, S_j}, y_i)] = \sigma^2 + \sigma^2 E_X[\text{tr}[(x_i x_i^T)(\mathbf{X}_{S_j}^T \mathbf{X}_{S_j})^{-1}]])$$
که در آن \(\sigma^2\) واریانس توزیع خطا است.

---