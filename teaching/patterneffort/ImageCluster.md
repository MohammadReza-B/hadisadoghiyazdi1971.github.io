---
layout: persian  # ุง single ุจุง ฺฉูุงุณ rtl-layout
classes: wide rtl-layout
dir: rtl
title: "ุฎูุดู ุจูุฏ ุชุตุงูุฑ"
permalink: /teaching/studenteffort/patterneffort/ImageCluster/
author_profile: true

header:
  overlay_image: "/assets/images/background.jpg"
  overlay_filter: 0.3
  overlay_color: "#5e616c"
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
---

# ุงุณุชุฎุฑุงุฌ ูฺฺฏ ุงุฒ ุตูุช

<div  dir="rtl">
<p >

<p> ููุณูุฏู : ูพุงุฑุณุง ุณู ฺ</p>
  <a href="mailto:p.sinichi@gmail.com">
p.sinichi@gmail.com  </a>
</p>
<p >

  ุฏุงูุดฺฏุงู ูุฑุฏูุณ ูุดูุฏ
  ูููุฏุณ ฺฉุงููพูุชุฑ
</p>

</div>

<div dir="rtl">


## ููุฑุณุช ูุทุงูุจ

<ul>
  <li><a href="#ููุฏูู">ููุฏูู</a></li>
  <li><a href="#ุงุฏฺฏุฑ-ุจุฏูู-ูุธุงุฑุช-ู-ุฎูุดูุจูุฏ">ุงุฏฺฏุฑ ุจุฏูู ูุธุงุฑุช ู ุฎูุดูโุจูุฏ</a></li>
  <li><a href="#ุงููุช-ุงุณุชุฎุฑุงุฌ-ูฺฺฏ">ุงููุช ุงุณุชุฎุฑุงุฌ ูฺฺฏ</a></li>
  <li><a href="#ุดุจฺฉู-ูุง-ุนุตุจ-cnn-ู-ูุฏู-vgg16">ุดุจฺฉู ูุง ุนุตุจ CNN ู ูุฏู VGG16</a></li>
  <li><a href="#ุชูุงุจุน-ุถุฑุฑ">ุชูุงุจุน ุถุฑุฑ</a>
    <ul>
      <li><a href="#1-ุชุงุจุน-ุถุฑุฑ-ูุฑุจุน-square-loss--l2">1. ุชุงุจุน ุถุฑุฑ ูุฑุจุน Square Loss / L2</a></li>
      <li><a href="#2-ุชุงุจุน-ุถุฑุฑ-ูุทูู-absolute-loss--l1">2. ุชุงุจุน ุถุฑุฑ ูุทูู Absolute Loss / L1</a></li>
      <li><a href="#3-ุชุงุจุน-huber-loss">3. ุชุงุจุน Huber Loss</a></li>
      <li><a href="#4-ุชุงุจุน-pseudo-huber-loss">4. ุชุงุจุน Pseudo-Huber Loss</a></li>
      <li><a href="#5-ุชุงุจุน-correntropy-loss">5. ุชุงุจุน Correntropy Loss</a></li>
      <li><a href="#6-ุชุงุจุน-epsilon-insensitive-loss">6. ุชุงุจุน Epsilon-Insensitive Loss</a></li>
    </ul>
  </li>
  <li><a href="#ุงุณุชูุงุฏู-ุงุฒ-ุชูุงุจุน-ุถุฑุฑ-ุจุฑุง-ุฎูุดู-ุจูุฏ">ุงุณุชูุงุฏู ุงุฒ ุชูุงุจุน ุถุฑุฑ ุจุฑุง ุฎูุดู ุจูุฏ</a>
    <ul>
      <li><a href="#1-ุฑุงูุญููุง-ุชุญูู-analytical-solutions">1. ุฑุงูโุญูโูุง ุชุญูู</a></li>
      <li><a href="#2-ุจูููุณุงุฒ-ุนุฏุฏ-numerical-optimization">2. ุจูููโุณุงุฒ ุนุฏุฏ</a></li>
    </ul>
  </li>
  <li><a href="#ุชูุงุจุน-ุจุง-ุฑุงู-ุญู-ูุญุงุณุจุงุช">ุชูุงุจุน ุจุง ุฑุงู ุญู ูุญุงุณุจุงุช</a>
    <ul>
      <li><a href="#ุชุงุจุน-l2-loss-square-loss--mean">ุชุงุจุน L2 Loss (Square Loss) โ Mean</a></li>
    </ul>
  </li>
  <li><a href="#ูพุงุฏู-ุณุงุฒ-ุฎูุดู-ุจูุฏ-ุจุง-ุงุณุชูุงุฏู-ุงุฒ-ุชูุงุจุน-ุถุฑุฑุฑ-ูุฎุชูู">ูพุงุฏู ุณุงุฒ ุฎูุดู ุจูุฏ ุจุง ุงุณุชูุงุฏู ุงุฒ ุชูุงุจุน ุถุฑุฑุฑ ูุฎุชูู</a>
    <ul>
      <li><a href="#ููุฏ-ฺฉุฑุฏู-ฺฉุชุงุจุฎุงูู-ูุง-ููุฑุฏ-ุงุณุชูุงุฏู-">ููุฏ ฺฉุฑุฏู ฺฉุชุงุจุฎุงูู ูุง ููุฑุฏ ุงุณุชูุงุฏู</a></li>
      <li><a href="#ููุฏ-ฺฉุฑุฏู-ูุฏู-vgg16-ุจุฑุง-ุงุณุชุฎุฑุงุฌ-ูฺฺฏ">ููุฏ ฺฉุฑุฏู ูุฏู vgg16 ุจุฑุง ุงุณุชุฎุฑุงุฌ ูฺฺฏ</a></li>
      <li><a href="#ุงุณุฎุฑุงุฌ-ูฺฺฏ-ุงุฒ-ูุฌููุนู-ุฏุงุฏู">ุงุณุฎุฑุงุฌ ูฺฺฏ ุงุฒ ูุฌููุนู ุฏุงุฏู</a></li>
      <li><a href="#ุชุนุฑู-ุชูุงุจุน-ุถุฑุฑ-ุฏุฑ-ฺฉุฏ">ุชุนุฑู ุชูุงุจุน ุถุฑุฑ ุฏุฑ ฺฉุฏ</a></li>
    </ul>
  </li>
  <li><a href="#ุงูฺฏูุฑุชู-ุฎูุดู-ุจูุฏ">ุงูฺฏูุฑุชู ุฎูุดู ุจูุฏ</a>
    <ul>
      <li><a href="#ูพุงุฏู-ุณุงุฒ">ูพุงุฏู ุณุงุฒ</a></li>
    </ul>
  </li>
  <li><a href="#ูุชุงุฌ-ุฎูุดู-ุจูุฏ">ูุชุงุฌ ุฎูุดู ุจูุฏ</a>
    <ul>
      <li><a href="#ุชุงุจุน-l2">ุชุงุจุน L2</a></li>
      <li><a href="#ุชุงุจุน-l1">ุชุงุจุน L1</a></li>
      <li><a href="#ุชุงุจุน-huber">ุชุงุจุน Huber</a></li>
      <li><a href="#ุชุงุจุน-pseudo-huber">ุชุงุจุน Pseudo-Huber</a></li>
      <li><a href="#ุชุงุจุน-correntropy">ุชุงุจุน Correntropy</a></li>
      <li><a href="#ุชุงุจุน-epsilon-insensitive">ุชุงุจุน Epsilon-Insensitive</a></li>
    </ul>
  </li>
  <li><a href="#ููุงุจุน">ููุงุจุน</a></li>
</ul>

---

</div>

## ููุฏูู

ุฏุฑ ุงู ูพุฑูฺูุ ุจู ุจุฑุฑุณ ู ูพุงุฏูโุณุงุฒ ุฑูุดโูุง ูุฎุชูู ุฎูุดูโุจูุฏ ุชุตุงูุฑ ุจุง ุงุณุชูุงุฏู ุงุฒ ุงุฏฺฏุฑ ุนูู ูโูพุฑุฏุงุฒู. ูุฏู ุงุตู ุงู ุงุณุช ฺฉู ุชุตุงูุฑ ุฑุง ุจุฑ ุงุณุงุณ ูุญุชูุง ุจุตุฑโุดุงู ุจู ฺฏุฑููโูุง ูุนูุงุฏุงุฑ ุชูุณู ฺฉูู. ุจุฑุง ุงู ููุธูุฑุ ุงุจุชุฏุง ุงุฒ ุดุจฺฉู ุนุตุจ ูพุดโุขููุฒุดโุฏุฏู VGG16 ุจุฑุง ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ุชุตุงูุฑ ุงุณุชูุงุฏู ูโฺฉูู. ุณูพุณ ุจุง ุจฺฉุงุฑฺฏุฑ ุชูุงุจุน ุถุฑุฑ ูุฎุชูู (ุงุฒ ุฌููู L1ุ L2ุ Huberุ Correntropy ู ...) ู ุชฺฉูฺฉโูุง ุจูููโุณุงุฒ ุนุฏุฏุ ุฎูุดูโุจูุฏ ุฑุง ุงูุฌุงู ูโุฏูู.

ุงู ูพุฑูฺู ูุดุงู ูโุฏูุฏ ฺฉู ุงูุชุฎุงุจ ุชุงุจุน ุถุฑุฑ ููุงุณุจ ฺฺฏููู ูโุชูุงูุฏ ุจุฑ ูุชุงุฌ ุฎูุดูโุจูุฏ ุชุฃุซุฑ ุจฺฏุฐุงุฑุฏ. ููฺูู ุชูุงูุช ุจู ุฑูุดโูุง ุชุญูู ู ุจูููโุณุงุฒ ุนุฏุฏ  ุฑุง ุจุฑุง ุงูุชู ูุฑุงฺฉุฒ ุจููู ุฎูุดูโูุง ุจุฑุฑุณ ูโฺฉูู.



## ุงุฏฺฏุฑ ุจุฏูู ูุธุงุฑุช ู ุฎูุดูโุจูุฏ

ุงุฏฺฏุฑ ุจุฏูู ูุธุงุฑุช ุจู ุชฺฉูฺฉโูุง ุงุทูุงู ูโุดูุฏ ฺฉู ุจุฏูู ูุงุฒ ุจู ุฏุงุฏูโูุง ุจุฑฺุณุจโุฏุงุฑุ ุงูฺฏููุง ู ฺฏุฑููโุจูุฏโูุง ุฑุง ุฏุฑ ุฏุงุฏูโูุง ฺฉุดู ูโฺฉููุฏ. ุฎูุดูโุจูุฏ ฺฉ ุงุฒ ุฑุงุฌโุชุฑู ุฑูุดโูุง ุงุฏฺฏุฑ ุจุฏูู ูุธุงุฑุช ุงุณุช ฺฉู ุฏุงุฏูโูุง ุฑุง ุจู ฺฏุฑููโูุง (ุฎูุดูโูุง) ุชูุณู ูโฺฉูุฏ ุจู ฺฏูููโุง ฺฉู ุนูุงุตุฑ ุฏุฑูู ูุฑ ุฎูุดู ุดุจุงูุช ุจุดุชุฑ ุจู ฺฉุฏฺฏุฑ ูุณุจุช ุจู ุนูุงุตุฑ ุฎูุดูโูุง ุฏฺฏุฑ ุฏุงุดุชู ุจุงุดูุฏ. ุจุง ุงุนูุงู ุฎูุดูโุจูุฏ ุจุฑ ุฑู ูฺฺฏโูุง ุงุณุชุฎุฑุงุฌโุดุฏู ุงุฒ ุชุตุงูุฑุ ูโุชูุงูู ุจู ุทูุฑ ุฎูุฏฺฉุงุฑ ุชุตุงูุฑ ุฑุง ุจุฑ ุงุณุงุณ ูุญุชูุง ุจุตุฑโุดุงู ุจู ฺฏุฑููโูุง ูุนูุงุฏุงุฑ ุณุงุฒูุงูุฏู ฺฉูู.

ุงู ูพุฑูฺู ุงูฺฏูุฑุชูโูุง ุฎูุดูโุจูุฏ ุชุนููโุงูุชู ุฑุง ุจุง ุงุณุชูุงุฏู ุงุฒ ุชูุงุจุน ุถุฑุฑ ูุฎุชูู ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ ฺฉู ุงูฺฉุงู ฺฏุฑููโุจูุฏ ุงูุนุทุงูโูพุฐุฑ ู ููุงูู ุชุตุงูุฑ ุฑุง ูุฑุงูู ูโุขูุฑุฏ. ูุชุงุฌ ุจูโุฏุณุชโุขูุฏู ุจูุด ุฏุฑุจุงุฑู ุณุงุฎุชุงุฑ ูุฌููุนู ุฏุงุฏู ุชุตูุฑ ุงุฑุงุฆู ูโุฏูุฏ ู ูุฏุฑุช ุชุฑฺฉุจ ุงุณุชุฎุฑุงุฌ ูฺฺฏ ุนูู ุจุง ุงุฏฺฏุฑ ุจุฏูู ูุธุงุฑุช ุฑุง ูุดุงู ูโุฏูุฏ.

 

## ุงููุช ุงุณุชุฎุฑุงุฌ ูฺฺฏ

ุชุตุงูุฑ ุจู ุทูุฑ ฺฉู ุฏุงุฏูโูุง ุจุง ุงุจุนุงุฏ ุจุงูุง ูุณุชูุฏ ฺฉู ูุนูููุงู ุดุงูู ูุฒุงุฑุงู ุง ููููโูุง ูพฺฉุณู ูโุจุงุดูุฏ. ุงุณุชูุงุฏูโ ูุณุชูู ุงุฒ ุงู ููุงุฏุฑ ูพฺฉุณู ุจุฑุง ูุธุงู ุงุฏฺฏุฑ ูุงุดูุ ูุงฺฉุงุฑุขูุฏ ู ุจูโูุฏุฑุช ูุคุซุฑ ุงุณุชุ ุฒุฑุง ุงู ููุงุฏุฑ ุงูฺฏููุง ุง ูุญุชูุง ูุนูุง ุฒุฑู ุชุตูุฑ ุฑุง ุจูโุฎูุจ ููุงุด ููโุฏููุฏ.
ุงุณุชุฎุฑุงุฌ ูฺฺฏุ ุชุตุงูุฑ ุฎุงู ุฑุง ุจู ููุงุดโูุง ูุดุฑุฏู ู ุขฺฏุงูุงููโุง ุชุจุฏู ูโฺฉูุฏ ฺฉู ุงุทูุงุนุงุช ุจุตุฑ ุงุณุงุณ ุฑุง ุฎูุงุตู ูโุณุงุฒูุฏ. ุงู ฺฉุงุฑ ุจุงุนุซ ูโุดูุฏ ูุธุงู ุจุนุฏ ูุงููุฏ ุทุจููโุจูุฏุ ุฎูุดูโุจูุฏ ู ุจุงุฒุงุจุ ููุงููโุชุฑ ู ุงุฒ ูุธุฑ ูุญุงุณุจุงุช ุงูฺฉุงูโูพุฐุฑุชุฑ ุดููุฏ.

## ุดุจฺฉู ูุง ุนุตุจ CNN ู ูุฏู VGG16

ุดุจฺฉูโูุง ุนุตุจ ฺฉุงููููุดู ุง ุจู ุงุฎุชุตุงุฑ (CNN) ฺฉ ุงุฒ ุงููุงุน ูุฏูโูุง ุงุฏฺฏุฑ ุนูู ูุณุชูุฏ ฺฉู ุจูโุทูุฑ ุฎุงุต ุจุฑุง ุฏุงุฏูโูุง ุชุตูุฑ ุทุฑุงุญ ุดุฏูโุงูุฏ. ุงู ุดุจฺฉูโูุง ุงุฒ ูุงูโูุง ฺฉุงููููุดู ุจุฑุง ุงุฏฺฏุฑ ุฎูุฏฺฉุงุฑ ูฺฺฏโูุง ุณูุณููโูุฑุงุชุจ )ุจุฒุฑฺฏ ุจู ฺฉูฺฺฉ) ุงุณุชูุงุฏู ูโฺฉููุฏ. ุงุฒ ูุจูโูุง ู ุงูฺฏููุง ุณุงุฏู ฺฏุฑูุชู ุชุง ุงุดฺฉุงู ู ุงุฌุณุงู ูพฺุฏูโุชุฑ. VGG16 ฺฉ ูุนูุงุฑ ูุนุฑูู ุงุฒ ููุน CNN ุงุณุช ฺฉู ุจู ุฏูู ุณุงุฏฺฏ ู ฺฉุงุฑุง ุจุงูุง ุดูุงุฎุชู ูโุดูุฏ. ุงู ูุฏู ุงุฒ ฑถ ูุงู ุจุง ูุฒูโูุง ูุงุจู ุงุฏฺฏุฑ ุชุดฺฉู ุดุฏู ู ุงุฒ ููุชุฑูุง ฺฉุงููููุดู ฺฉูฺฺฉ (ณรณ) ุงุณุชูุงุฏู ูโฺฉูุฏ. VGG16 ุจุฑ ุฑู ูุฌููุนูโุฏุงุฏูโ ุจุฒุฑฺฏ ImageNet ฺฉู ุดุงูู ูููู ูุง ุชุตูุฑ ุฑูุฒูุฑู ุงุฒ ูพุด ุขููุฒุด ุฏุงุฏู ุดุฏู ุงุณุช ู ุจู ููู ุฏูู ูุงุฏุฑ ุงุณุช ูฺฺฏโูุง ุบู ู ูุงุจู ุชุนูู ุฑุง ุงุฒ ุชุตุงูุฑ ุฌุฏุฏ ุงุณุชุฎุฑุงุฌ ฺฉูุฏ.

ุฏุฑ ุงู ูพุฑูฺูุ ุงุฒ VGG16 ุจูโุนููุงู ุงุณุชุฎุฑุงุฌโฺฉููุฏูโ ูฺฺฏโูุง (Feature Extractor) ุงุณุชูุงุฏู ูโุดูุฏ. ุจูโุฌุง ุงุณุชูุงุฏู ุงุฒ ูุฏู ุจุฑุง ุทุจููโุจูุฏุ ุฎุฑูุฌ ูุงูโูุง ฺฉุงููููุดู ุขู ุจุฑุง ุจูโุฏุณุชโุขูุฑุฏู ุจุฑุฏุงุฑ ูฺฺฏ ูุฑ ุชุตูุฑ ุจู ฺฉุงุฑ ฺฏุฑูุชู ูโุดูุฏ. ุงู ุจุฑุฏุงุฑูุง ูููโุชุฑู ูฺฺฏโูุง ุจุตุฑ ุชุตุงูุฑ ุฑุง ุฏุฑ ุจุฑ ูโฺฏุฑูุฏ ู ูุจูุง ุจุฑุง ุฎูุดูโุจูุฏ ุฏุงุฏูโูุง ูุฑุงูู ูโฺฉููุฏ.

<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/fx-image2.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
ุงุณุชูุงุฏู ุงุฒ ุดุจฺฉู ุนุตุจ vgg16 ุจุฑุง ุงุณุชุฎุฑุงุฌ ูฺฺฏ

</div>

<br>
<br>
ุจุฑุง ุฏุฑฺฉ ุจูุชุฑ ูฺฺฏ ูุง ูุชูุงู ุชุนุฏุงุฏ ุงุฒ ูฺฺฏ ูุง ุงุณุชุฎุฑุงุฌ ุดุฏู ุงุฒ ุชุตุงูุฑ ุฑุง ูุดุงูุฏู ฺฉุฑุฏ

<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/org_ant.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
ุชุตูุฑ ููููู ููุฑุฏ ุงุณุชูุงุฏู ุจุฑุง ุงุณุชุฎุฑุงุฌ ูฺฺฏ
</div>

<br>
<br>
<br>

ุฏุฑ ุชุตูุฑ ุฒุฑ ูุชูุงู ุชุนุฏุงุฏ ุงุฒ ูฺฺฏ ูุง ุงุณุชุฎุฑุงุฌ ุดุฏู ุฑุง ูุดุงูุฏู ฺฉุฑุฏ :

<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/layer_1_overview.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
</div>
<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/layer_1_overview.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
ููููู ุงุฒ ูฺฺฏ ูุง ุงุณุชุฎุฑุงุฌ ุดุฏู ุงุฒ ุดุจฺฉู ุนุตุจ CNN</div>

<br>

```python
import torch
import torchvision.models as models
import torchvision.transforms as T
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
#  Load VGG16
vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features.eval()
layers_to_tap = [1,2,3, 8, 15, 22, 29] 
features = {}
def save_activation(name):
    def hook(module, inp, out):
        features[name] = out.detach().cpu()
    return hook
for idx in layers_to_tap:
    vgg[idx].register_forward_hook(save_activation(f"layer_{idx}"))
#  Preprocess
tfms = T.Compose([
    T.Resize(256),
    T.CenterCrop(224),
    T.ToTensor(),
    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
])
img_path = ""  
img = Image.open(img_path).convert("RGB")
plt.imshow(img)
plt.axis(False)
plt.show()
x = tfms(img).unsqueeze(0)
with torch.no_grad():
    vgg(x)
def normalize_channel(arr):
    arr = arr - arr.mean()
    arr = arr / (arr.std() + 1e-5)
    arr = arr * 64 + 128
    return np.clip(arr, 0, 255).astype("uint8")
channels_to_show = 8
for name, fmap in features.items():
    #[1, C, H, W]
    fmap = fmap[0]
    n = min(channels_to_show, fmap.shape[0])
    cols = 4
    rows = int(np.ceil(n / cols))
    plt.figure(figsize=(cols * 4.2, rows * 4.2))
    plt.suptitle(f"{name} โ feature maps", y=0.98, fontsize=12)
    for i in range(n):
        ax = plt.subplot(rows, cols, i + 1)
        chan = fmap[i].cpu().numpy()
        norm_img = normalize_channel(chan)
        ax.imshow(norm_img, cmap="viridis")
        ax.set_xticks([]); ax.set_yticks([])
        ax.set_title(f"ch {i}", fontsize=8)
    plt.tight_layout()
    plt.show()
    plt.close()
```

---

ุฏุฑ ุงู ูพุฑูฺู ูุฏู ูุง ุงุนูุงู ุฑูุด ูุง ูุฎุชูู ุจููู ุณุงุฒ ู ุชูุงุจุน ุถุฑุฑ ูุฎุชูู ุจุฑุง ุฎูุดู ุจูุฏ ุชุตุงูุฑ ุงุณุช

ุจู ุนููุงู ููููู ุฏุฑ ุงู ูุซุงู ุ ุงุฒ ุชุนุฏุงุฏ ุง ุชุตุงูุฑ ุญูุงูุงุช ุงุณุชูุงุฏู ูฺฉูู ู ุณุน ูฺฉูู ุจุง ุงุณุชูุงุฏู ุงุฒ ูฺฺฏ ูุง ุงุณุชุฎุฑุงุฌ ุดุฏู ุงุฒ ุขู ูุง ุ ุฎูุดู ุจูุฏ ุฑุง ุงูุฌุงู ุฏูู.

ุฏุฑ ุดฺฉู ุฒุฑ ฺูุฏ ููููู ุงุฒ ุชุตุงูุฑ ุฏุชุงุณุช ุฑุง ูุชูุงู ูุดุงูุฏู ฺฉุฑุฏ :

<div style="display: flex; justify-content: center; align-items: center; gap: 10px; flex-wrap: wrap;">
    <img src="/assets/patterneffort/ImageCluster/data_sample (3).jpg" alt="IPS1" style="width: 24%; height: auto; object-fit: contain;">
    <img src="/assets/patterneffort/ImageCluster/data_sample (1).jpg" alt="IPS1" style="width: 24%; height: auto; object-fit: contain;">
    <img src="/assets/patterneffort/ImageCluster/data_sample (2).jpg" alt="IPS1" style="width: 24%; height: auto; object-fit: contain;">
    <img src="/assets/patterneffort/ImageCluster/data_sample (4).jpg" alt="IPS1" style="width: 24%; height: auto; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
ููููู ุง ุงุฒ ุชุตุงูุฑ ุฏุฑ ูุฌููุนู ุฏุงุฏู ูุง
</div>

# ุชูุงุจุน ุถุฑุฑ

ุฏุฑ ุงู ูุณูุช ุจู ูุนุฑู ุชูุงุจุน ุถุฑุฑ ฺฉู ุจุฑุง ุฎูุดู ุจูุฏ ุฏุฑ ุงู ุชูุฑู ุงุณุชูุงุฏู ูฺฉูู ุฎูุงูู ูพุฑุฏุงุฎุช :

## 1. ุชุงุจุน ุถุฑุฑ  Square Loss / L2

**ูุธุฑู:** ุถุฑุฑ ูุฑุจุน ูุฌููุน ูุฑุจุน ูุงุตููโูุง ุฑุง ฺฉููู ูโฺฉูุฏ. ุงู ุฑุงุฌโุชุฑู ุชุงุจุน ุถุฑุฑ (ุฏุฑ k-means ุงุณุชูุงุฏู ูโุดูุฏ) ุงุณุช ุงูุง ุจู ุฏูู ูุฑุจุน ุดุฏู ุฎุทุงูุง ุจุฒุฑฺฏุ ุจู ุฏุงุฏูโูุง ูพุฑุช ุจุณุงุฑ ุญุณุงุณ ุงุณุช.

**ูุฑููู:**
<div dir="ltr">


$$L(x, \mu) = \sum (x - \mu)^2$$

</div>

**ููุงุฏฺฏุฐุงุฑ:**

- $x$โ: ุจุฑุฏุงุฑ ุฏุงุฏู (ููุทู ุฏุงุฏู)
- $\mu$: ูุฑฺฉุฒ ุฎูุดู (ููุงูุฏู)
- $(x - \mu)^2$: ูุฑุจุน ูุงุตูู ุงููุฏุณ

---

## 2. ุชุงุจุน ุถุฑุฑ  Absolute Loss / L1

**ูุธุฑู:** ุถุฑุฑ ูุทูู ูุฌููุน ูุงุตููโูุง ูุทูู ุฑุง ฺฉููู ูโฺฉูุฏ. ูุณุจุช ุจู ุถุฑุฑ ูุฑุจุน ุจู ุฏุงุฏูโูุง ูพุฑุช ููุงููโุชุฑ ุงุณุช ฺูู ุฎุทุงูุง ุจู ุตูุฑุช ุฎุท ุฑุดุฏ ูโฺฉููุฏ ูู ุฏุฑุฌู ุฏูู.

**ูุฑููู:**

<div dir="ltr">

$$L(x, \mu) = \sum |x - \mu|$$

</div>

**ููุงุฏฺฏุฐุงุฑ:**

- $x$: ุจุฑุฏุงุฑ ุฏุงุฏู
- $\mu$: ูุฑฺฉุฒ ุฎูุดู
- $|x - \mu|$: ูุงุตูู ูููุชู (Manhattan distance)

## 3. ุชุงุจุน Huber Loss

**ูุธุฑู:** ุถุฑุฑ ููุจุฑ ุจูุชุฑูโูุง ุฏู ุฏูุง ุฑุง ุชุฑฺฉุจ ูโฺฉูุฏ - ุจุฑุง ุฎุทุงูุง ฺฉูฺฺฉ ุฏุฑุฌู ุฏูู (ูุซู L2) ู ุจุฑุง ุฎุทุงูุง ุจุฒุฑฺฏ ุฎุท (ูุซู L1) ุงุณุช. ุงู ุจุงุนุซ ูโุดูุฏ ุฏุฑ ุจุฑุงุจุฑ ุฏุงุฏูโูุง ูพุฑุช ููุงูู ุจุงุดุฏ ู ุฏุฑ ุนู ุญุงู ุฏุฑ ูุฒุฏฺฉ ูููู ูุฑู ุจุงุดุฏ.

**ูุฑููู:**

<div dir="ltr">

$$
L(x, \mu) = \begin{cases}
\frac{1}{2}(x-\mu)^2 & \text{if } |x-\mu| \leq \delta \\
\delta(|x-\mu| - \frac{\delta}{2}) & \text{otherwise}
\end{cases}
$$
</div>

**ููุงุฏฺฏุฐุงุฑ ู ูพุงุฑุงูุชุฑูุง:**

- $x$: ุจุฑุฏุงุฑ ุฏุงุฏู
- $\mu$: ูุฑฺฉุฒ ุฎูุดู
- $\delta$: ูพุงุฑุงูุชุฑ ุขุณุชุงูู ฺฉู ููุทู ุงูุชูุงู ุจู ุฑูุชุงุฑ ุฏุฑุฌู ุฏูู ู ุฎุท ุฑุง ฺฉูุชุฑู ูโฺฉูุฏ
  - ุจุฑุง $|x-\mu| \leq \delta$: ุฑูุชุงุฑ ุฏุฑุฌู ุฏูู (ูุดุงุจู L2)
  - ุจุฑุง $|x-\mu| > \delta$: ุฑูุชุงุฑ ุฎุท (ูุดุงุจู L1)

---

## 4. ุชุงุจุน Pseudo-Huber Loss

**ูุธุฑู:** ุชูุฑุจ ูุฑู ุถุฑุฑ ููุจุฑ ฺฉู ุฏุฑ ููู ุฌุง ูุดุชูโูพุฐุฑ ุงุณุช. ุฑูุชุงุฑ ูุดุงุจู ุถุฑุฑ ููุจุฑ ุฏุงุฑุฏ ุงูุง ุจุฏูู ุงูุชูุงู ูุงฺฏูุงู.

**ูุฑููู:**



<div dir="ltr">

$$
L(x, \mu) = \sum \delta^2 \left(\sqrt{1 + \left(\frac{x-\mu}{\delta}\right)^2} - 1\right)
$$

</div>


**ููุงุฏฺฏุฐุงุฑ ู ูพุงุฑุงูุชุฑูุง:**

- $x$: ุจุฑุฏุงุฑ ุฏุงุฏู
- $\mu$: ูุฑฺฉุฒ ุฎูุดู
- $\delta$: ูพุงุฑุงูุชุฑ ูุฑู ฺฉู ุฏุฑุฌู ุชูุฑุจ ุฑุง ฺฉูุชุฑู ูโฺฉูุฏ
  - ููุงุฏุฑ ฺฉูฺฺฉ $\delta$: ุฑูุชุงุฑ ูุฒุฏฺฉ ุจู L1
  - ููุงุฏุฑ ุจุฒุฑฺฏ $\delta$: ุฑูุชุงุฑ ูุฒุฏฺฉ ุจู L2

---

## 5. ุชุงุจุน (Correntropy Loss)

**ูุธุฑู:** ุจุฑ ุงุณุงุณ ฺฉุฑูู ฺฏุงูุณ ุงุฒ ูุธุฑู ุงุทูุงุนุงุช. ุดุจุงูุช ุฑุง ุงูุฏุงุฒูโฺฏุฑ ูโฺฉูุฏ ูู ูุงุตูู ู ุฏุฑ ุจุฑุงุจุฑ ุฏุงุฏูโูุง ูพุฑุช ุจุณุงุฑ ููุงูู ุงุณุช. ููุงุท ุฏูุฑ ุงุฒ ูุฑฺฉุฒ ุชูุฑุจุงู ูฺ ุณูู ุฏุฑ ุถุฑุฑ ูุฏุงุฑูุฏ.

**ูุฑููู:**
<div dir="ltr">

$$L(x, \mu) = 1 - \exp\left(-\frac{\sum(x-\mu)^2}{2\sigma^2}\right)$$

</div>

**ููุงุฏฺฏุฐุงุฑ ู ูพุงุฑุงูุชุฑูุง:**

- $x$: ุจุฑุฏุงุฑ ุฏุงุฏู
- $\mu$: ูุฑฺฉุฒ ุฎูุดู
- $\sigma$: ูพุงุฑุงูุชุฑ ุนุฑุถ ฺฉุฑูู (kernel bandwidth)
  - ููุงุฏุฑ ฺฉูฺฺฉ $\sigma$: ุญุณุงุณุช ุจุดุชุฑ ุจู ูุงุตููโูุง ฺฉูฺฺฉ
  - ููุงุฏุฑ ุจุฒุฑฺฏ $\sigma$: ุชุญูู ุจุดุชุฑ ูุณุจุช ุจู ุฏุงุฏูโูุง ูพุฑุช

---

## 6. ุชุงุจุน (Epsilon-Insensitive Loss)

**ูุธุฑู:** ุฏุฑ ูุงุดูโูุง ุจุฑุฏุงุฑ ูพุดุชุจุงู (SVM) ุงุณุชูุงุฏู ูโุดูุฏ. ุงู ุชุงุจุน ุถุฑุฑ ุฎุทุงูุง ฺฉูฺฺฉุชุฑ ุงุฒ ุงูพุณููู ุฑุง ูุงุฏุฏู ูโฺฏุฑุฏ ู ููุท ููุงุท ุฏูุฑ ุงุฒ ูุฑฺฉุฒ ุฑุง ุฌุฑูู ูโฺฉูุฏ. ุฒูุงู ุฎูุจ ุงุณุช ฺฉู ูโุฎูุงูุฏ ุชุบุฑุงุช ฺฉูฺฺฉ ุฑุง ูุงุฏุฏู ุจฺฏุฑุฏ.

**ูุฑููู:**

<div dir="ltr">

$$ L(x, \mu) = \max(0, \|x - \mu\| - \epsilon) $$

</div>

**ููุงุฏฺฏุฐุงุฑ ู ูพุงุฑุงูุชุฑูุง:**

- $x$: ุจุฑุฏุงุฑ ุฏุงุฏู
- $\mu$: ูุฑฺฉุฒ ุฎูุดู
- $\|x - \mu\|$: ูุฑู ุงููุฏุณ (ูุงุตูู)
- $\epsilon$: ูพุงุฑุงูุชุฑ ููุทูู ุจโุญุณุงุณ
  - ุจุฑุง $\|x - \mu\| \leq \epsilon$: ุถุฑุฑ ุตูุฑ ุงุณุช
  - ุจุฑุง $\|x - \mu\| > \epsilon$: ุถุฑุฑ ุจู ุตูุฑุช ุฎุท ุงูุฒุงุด ูโุงุจุฏ

---

# ุงุณุชูุงุฏู ุงุฒ ุชูุงุจุน ุถุฑุฑ ุจุฑุง ุฎูุดู ุจูุฏ

ุฏุฑ ูุณุฆููโ ุฎูุดูโุจูุฏุ ุจุงุฏ ยซูุฑฺฉุฒยป ูุฑ ุฎูุดู ุฑุง ุจุงุจู ฺฉู ูุฌููุน ุฒุงูโูุง ุฑุง ฺฉููู ฺฉูุฏ. ุจุณุชู ุจู ููุน ุชุงุจุน ุถุฑุฑ ุงู ฺฉุงุฑ ูโุชูุงูุฏ ุจู ฺฉ ุงุฒ ุฏู ุฑูุด ุฒุฑ ุงูุฌุงู ุดูุฏ:

---

#### 1. **ุฑุงูโุญูโูุง ุชุญูู (Analytical Solutions)**

ุจุฑุฎ ุชุงุจุนโูุง ุถุฑุฑ ูุฑููู ุฑุงุถ ูุดุฎุต ุจุฑุง ูุฑฺฉุฒ ุจููู ุฏุงุฑูุฏ:


<div dir="rtl">
ุจู ุนููุงู ูุซุงู ุชุงุจุน ุถุฑุฑ L2 ุฑุง ูุชูุงู ูุณุชูู ุจุง ูุงูฺฏู ุญุณุงุจ ฺฉุฑุฏ
<br>

  

</div>

$ \mu^* = \frac{1}{n}\sum_{i=1}^n x_i $

 

<div dir="rtl">

<div dir="rtl">
ุจู ุนููุงู ูุซุงู ุชุงุจุน ุถุฑุฑ L1 ุฑุง ูุชูุงู ูุณุชูู ุจุง ูุงูู ุญุณุงุจ ฺฉุฑุฏ
<br>

  

</div>   
</div>

$ \mu^* = \text{median}(x_1, x_2, ..., x_n) $
 
---

#### 2. ุจูููโุณุงุฒ ุนุฏุฏ (Numerical Optimization)

ุจุดุชุฑ ุชุงุจุนโูุง ุฒุงู ุฏฺฏุฑ (ูุซู Huberุ Correntropy ู ...) ูุฑููู ุจุณุชูโุง ุจุฑุง $\mu^*$ ูุฏุงุฑูุฏุ
ุจูุงุจุฑุงู ุจุงุฏ ุงุฒ ุฑูุดโูุง ุนุฏุฏ ุจุฑุง ุงูุชู ูุฑฺฉุฒ ุจููู ุงุณุชูุงุฏู ฺฉูู.

---

**ูุญููโ ฺฉุงุฑ `scipy.optimize.minimize`:**

```python
result = minimize(objective_function, initial_guess, method='BFGS')
```

<div dir="rtl" style="text-align: right;">
- ุชุงุจุน ูุฏู (Objective Function): ุชุงุจุน ฺฉู ูโุฎูุงูู ฺฉููู ฺฉูู (ูุฌููุน ุฒุงูโูุง)  
- ุญุฏุณ ุงููู (Initial Guess): ููุทูู ุดุฑูุน ุฌุณุชโูุฌู (ูุนูููุงู ุงุฒ ูุงูฺฏู ุฏุงุฏูโูุง ุงุณุชูุงุฏู ูโุดูุฏ)  
- method='BFGS': ุงูฺฏูุฑุชู ูุจุชู ุจุฑ ฺฏุฑุงุฏุงู ฺฉู ูุฑุงุญู ุฒุฑ ุฑุง ุท ูโฺฉูุฏ:

1. ุงุฒ ุญุฏุณ ุงููู ุขุบุงุฒ ูโฺฉูุฏ
2. ฺฏุฑุงุฏุงู (ุฌูุช ุจุดุชุฑู ฺฉุงูุด) ุฑุง ูุญุงุณุจู ูโฺฉูุฏ
3. ุฏุฑ ุขู ุฌูุช ฺฉ ฺฏุงู ุจุฑูโุฏุงุฑุฏ
4. ุงู ูุฑุงูุฏ ุฑุง ุชุง ููฺฏุฑุง ุจู ุญุฏุงูู ุชฺฉุฑุงุฑ ูโฺฉูุฏ
</div>

#### ููุงุณู ู ููุงุฒูู (Trade-offs):

<div dir="rtl" style="text-align: right;">
- ุชุญูู: ูุญุงุณุจูู ุณุฑุน ู ุฏููุ ุจุฏูู ุฎุทุง ุชูุฑุจ  
- ุนุฏุฏ: ฺฉูุฏุชุฑุ ุงูุง ุชููุง ฺฏุฒููู ููฺฉู ุฒูุงู ฺฉู ูุฑููู ุจุณุชู ูุฌูุฏ ูุฏุงุฑุฏ
</div>

ุจู ุนููุงู ูุซุงู ฺฉ ููููู ุงุฒ ฺฉุงุฑฺฉุฑุฏ ุงู ุฑูุด ุฑุง ุจุฑุง ฺฉ ุชุงุจุน ุถุฑุฑ ูุชูุงู ูุดุงูุฏู ฺฉุฑุฏ
ุฏุฑ ุงุจุชุฏุง ุงุฒ ฺฉ ููุทู ุชุตุงุฏู ุดุฑูุน ฺฉุฑุฏู ู ุจู ุณูุช ูุฎุงูู ฺฏุฑุงุฏุงู ู ุฑูู (โฺฏุฑุงุฏุงู ูุฒูู)
ุฏุฑ ูุงูุน ููุทู ุงูุชุฎุงุจ ุดุฏู ุ ููุทู ุง ุจุง ุญุฏุงูุงู ูุงุตูู ุจุง ุณุงุฑ ููุงุท ุงุณุช.

<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/sgd.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
</div>
---

# ุชูุงุจุน ุจุง ุฑุงู ุญู ูุญุงุณุจุงุช

## ุชุงุจุน L2 Loss (Square Loss) โ Mean

ฺฉ ุงุฒ ุชูุงุจุน ุจุณุงุฑ ูพุฑฺฉุงุฑุจุฑุฏ ููุฑุฏ ุงุณุชูุงุฏู ุฏุฑ ุงูฺฏูุฑุชู ูุง ูุซู k-means

### ฺฏุงู ฑ: ูุดุชูโฺฏุฑ ูุณุจุช ุจู $\mu^*$

<div dir="ltr">

$
\frac{dL}{d\mu} = \frac{d}{d\mu} \sum_{i=1}^{n} (x_i - \mu)^2
$

</div>



ุจุง ุงุณุชูุงุฏู ุงุฒ ูุงุนุฏูโ ุฒูุฌุฑูโุง:

$
\frac{dL}{d\mu}
= \sum_{i=1}^{n} \frac{d}{d\mu}(x_i - \mu)^2
= \sum_{i=1}^{n} 2(x_i - \mu)(-1)$

$\frac{dL}{d\mu} = -2\sum_{i=1}^{n} (x_i - \mu)$

---

### ฺฏุงู ฒ: ุจุฑุงุจุฑ ุตูุฑ ูุฑุงุฑ ุฏุงุฏู ูุดุชู (ุดุฑุท ูุงุฒู ุจุฑุง ฺฉููู)

$-2\sum_{i=1}^{n} (x_i - \mu) = 0$

$\sum_{i=1}^{n} (x_i - \mu) = 0$

---

### ฺฏุงู ณ: ุญู ุจุฑุง $\mu^*$

$\sum_{i=1}^{n} x_i - \sum_{i=1}^{n} \mu = 0$

$\sum_{i=1}^{n} x_i - n\mu = 0$

$\mu^* = \frac{1}{n}\sum_{i=1}^{n} x_i$

---

# ูพุงุฏู ุณุงุฒ ุฎูุดู ุจูุฏ ุจุง ุงุณุชูุงุฏู ุงุฒ ุชูุงุจุน ุถุฑุฑุฑ ูุฎุชูู

## ููุฏ ฺฉุฑุฏู ฺฉุชุงุจุฎุงูู ูุง ููุฑุฏ ุงุณุชูุงุฏู :

```python
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
import os
import numpy as np
from scipy.optimize import minimize
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from tqdm import tqdm
```

## ููุฏ ฺฉุฑุฏู ูุฏู vgg16 ุจุฑุง ุงุณุชุฎุฑุงุฌ ูฺฺฏ

```python

#pre-trained VGG16 model
vgg16 = models.vgg16(pretrained=True)
model = nn.Sequential(*list(vgg16.features.children()))
model.eval()



image preprocessing
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
```

## ุงุณุฎุฑุงุฌ ูฺฺฏ ุงุฒ ูุฌููุนู ุฏุงุฏู

ุฏุฑ ุงู ูุณูุช ุงุฒ ุนฺฉุณ ูุง ููุฌูุฏ ุฏุฑ ุฏุชุงุณุช ุ ุงุณุชุฎุฑุงุฌ ูฺฺฏ ุงูุฌุงู ู ุฏูู.

```python
def extract_features(img_path, model, device='cpu'):
    image = Image.open(img_path).convert("RGB")
    input_tensor = preprocess(image).unsqueeze(0).to(device)

    with torch.no_grad():
        features = model(input_tensor)

    # Global average pooling to get a 512-dim vector
    global_pooled = torch.mean(features, dim=(2, 3))
    return global_pooled.squeeze().cpu().numpy()


def extract_features_from_folder(folder_path, model, device):
    """
    Extract features from all images in a folder.

    Args:
        folder_path: Path to folder containing images
        model: Pre-trained VGG16 model
        device: Device to run the model on

    Returns:
        features_dict: Dictionary mapping filenames to feature vectors
        image_files: List of image filenames
    """
    features_dict = {}
    image_files = [f for f in os.listdir(folder_path)
                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    image_files.sort()

    print(f"Found {len(image_files)} images in '{folder_path}'")

    for file in image_files:
        img_path = os.path.join(folder_path, file)
        print(f"  Extracting features from: {file}")
        features = extract_features(img_path, model, device)
        features_dict[file] = features

    print("โ Feature extraction complete!")
    return features_dict, image_files


# Extract features from all images
image_folder = "folder name"
features_dict, image_files = extract_features_from_folder(image_folder, model)

# Convert to numpy array for clustering
features_list = list(features_dict.values())
data = np.stack(features_list)

print(f"\n๐ Data shape: {data.shape}")
print(f"   โ {data.shape[0]} images, each with {data.shape[1]} features")
```

## ุชุนุฑู ุชูุงุจุน ุถุฑุฑ ุฏุฑ ฺฉุฏ

ุฏุฑ ุงู ูุณูุช ฺฉ ุชุงุจุน ฺฉู ุชุนุฑู ูฺฉูู ฺฉู ุจู ุนููุงู ูุฑูุฏ ุงุณู ุชุงุจุน ุฑุง ฺฏุฑูุชู ู ูุฑููู ุขู ุฑุง ุจุฑ ู ฺฏุฑุฏุงูู

```python

def get_loss_func(loss_type, params={}):
    """
    Returns a loss function based on the specified type.

    Args:
        loss_type: Name of the loss function
        params: Dictionary of parameters for the loss function

    Returns:
        Loss function that takes (x, mu) and returns a scalar
    """

    if loss_type == 'square':
       
        def loss(x, mu):
            return np.sum((x - mu)**2)
        return loss

    elif loss_type == 'absolute':
     
        def loss(x, mu):
            return np.sum(np.abs(x - mu))
        return loss

    elif loss_type == 'huber':
      
        delta = params.get('delta', 1.0)
        def loss(x, mu):
            res = np.abs(x - mu)
            return np.sum(np.where(res <= delta,
                                   0.5 * res**2,
                                   delta * (res - 0.5 * delta)))
        return loss

    elif loss_type == 'pseudo_huber':
        
        delta = params.get('delta', 1.0)
        def loss(x, mu):
            res = x - mu
            return np.sum(delta**2 * (np.sqrt(1 + (res / delta)**2) - 1))
        return loss

    elif loss_type == 'correntropy':
        
        sigma = params.get('sigma', 1.0)
        def loss(x, mu):
            d2 = np.sum((x - mu)**2)
            return 1 - np.exp(-d2 / (2 * sigma**2))
        return loss

    elif loss_type == 'epsilon_insensitive':
        
        epsilon = params.get('epsilon', 1.0)
        def loss(x, mu):
            d = np.linalg.norm(x - mu)
            return max(0, d - epsilon)
        return loss

```

# ุงูฺฏูุฑุชู ุฎูุดู ุจูุฏ

ุชูุถุญ ฺฉู ุฑุงุฌุจ ุฎูุดู ุจูุฏ .....

## ูพุงุฏู ุณุงุฒ

```python

def update_center(points, loss_type, params):
    if len(points) == 0:
        return np.zeros(points.shape[1])
    if loss_type == 'square':
        return np.mean(points, axis=0)
    elif loss_type == 'absolute':
        return np.median(points, axis=0)
    else:
        def objective(mu):
            loss_func = get_loss_func(loss_type, params)
            total_loss = sum(loss_func(x, mu) for x in points)
            return total_loss
        
        initial_guess = np.mean(points, axis=0)
        result = minimize(
            objective,              
            initial_guess,          
            method='BFGS',          
            options={'maxiter': 20} 
        )
        return result.x
```

```python

def generalized_clustering(data, k, loss_type, params={}, max_iter=3, tol=1e-4):
    n, d = data.shape
    centers = data[np.random.choice(n, k, replace=False)]
    prev_shift = np.inf
    for iter in range(max_iter):
        # Assign each point to nearest center
        dists = np.array([[get_loss_func(loss_type, params)(data[i], centers[j])
                          for j in range(k)]
                         for i in range(n)])
        labels = np.argmin(dists, axis=1)

        #  Update centers
        new_centers = np.zeros((k, d))
        for j in tqdm(range(k), desc=f"Updating centers (iter {iter+1})"):
            points_j = data[labels == j]
            if len(points_j) > 0:
                new_centers[j] = update_center(points_j, loss_type, params)
        # Check convergence
        shift = np.sum(np.linalg.norm(new_centers - centers, axis=1))
        centers = new_centers
        if shift < tol or abs(shift - prev_shift) < 1e-6:
            print(f"  Converged after {iter+1} iterations")
            break
        prev_shift = shift
    return centers, labels

```

```python
def find_closest_image(center, points, indices, loss_func):
    min_dist = np.inf
    closest_idx = -1
    for idx, p in zip(indices, points):
        dist = loss_func(p, center)
        if dist < min_dist:
            min_dist = dist
            closest_idx = idx
    return closest_idx


def plot_representatives(loss_type, centers, labels, image_files, folder_path, loss_func):
    fig, axs = plt.subplots(1, len(centers), figsize=(5 * len(centers), 5))
    fig.suptitle(f'Representative Images - {loss_type.upper()} Loss', fontsize=16, fontweight='bold')
    for j, center in enumerate(centers):
        # Find all images in this cluster
        cluster_indices = [i for i in range(len(labels)) if labels[i] == j]
        cluster_points = data[cluster_indices]
        if len(cluster_points) > 0:
            # Find the image closest to the center
            closest_i = find_closest_image(center, cluster_points, cluster_indices, loss_func)
            img_file = image_files[closest_i]
            img_path = os.path.join(folder_path, img_file)
            img = mpimg.imread(img_path)
            if len(centers) == 1:
                axs.imshow(img)
                axs.set_title(f'Representative {j+1}\n{img_file}', fontsize=12)
                axs.axis('off')
            else:
                axs[j].imshow(img)
                axs[j].set_title(f'Representative {j+1}\n{img_file}', fontsize=12)
                axs[j].axis('off')
    plt.tight_layout()
    plt.show()

```

# ูุชุงุฌ ุฎูุดู ุจูุฏ

### ุชุงุจุน L2

```python

loss_type = 'square'
params = params_dict[loss_type]

z=(f"๐ Running clustering with {loss_type.upper()} loss...")
centers, labels = generalized_clustering(data, k, loss_type, params)



loss_func = get_loss_func(loss_type, params)
plot_representatives(loss_type, centers, labels, image_files, image_folder, loss_func,z)

```

<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/SQUARE loss.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
ุญุงุตู ุฎูุดู ุจูุฏ ุจุง ุชุงุจุน ุถุฑุฑ L2 ู ููุงุด 3 ูุฑฺฉุฒ ุฎูุดู ุจุฑุชุฑ
</div>
<!--  -->

### ุชุงุจุน L1

```python

loss_type = 'absolute'
params = params_dict[loss_type]

z=(f"๐ Running clustering with {loss_type.upper()} loss...")
centers, labels = generalized_clustering(data, k, loss_type, params)



loss_func = get_loss_func(loss_type, params)
plot_representatives(loss_type, centers, labels, image_files, image_folder, loss_func,z)

```

<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/ABSOLUTE loss.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
ุญุงุตู ุฎูุดู ุจูุฏ ุจุง ุชุงุจุน ุถุฑุฑ l1 ู ููุงุด 3 ูุฑฺฉุฒ ุฎูุดู ุจุฑุชุฑ
</div>
<!--  -->
<!--  -->

### ุชุงุจุน Huber

```python

loss_type = 'huber'
params = params_dict[loss_type]

z=(f"๐ Running clustering with {loss_type.upper()} loss...")
centers, labels = generalized_clustering(data, k, loss_type, params)



loss_func = get_loss_func(loss_type, params)
plot_representatives(loss_type, centers, labels, image_files, image_folder, loss_func,z)

```

<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/HUBER loss.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
ุญุงุตู ุฎูุดู ุจูุฏ ุจุง ุชุงุจุน ุถุฑุฑ huber ู ููุงุด 3 ูุฑฺฉุฒ ุฎูุดู ุจุฑุชุฑ
</div>

### ุชุงุจุน Pseudo-Huber

```python

loss_type = 'pseudo-huber'
params = params_dict[loss_type]

z=(f"๐ Running clustering with {loss_type.upper()} loss...")
centers, labels = generalized_clustering(data, k, loss_type, params)


loss_func = get_loss_func(loss_type, params)
plot_representatives(loss_type, centers, labels, image_files, image_folder, loss_func,z)

```

<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/PSEUDO_HUBER loss.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
ุญุงุตู ุฎูุดู ุจูุฏ ุจุง ุชุงุจุน ุถุฑุฑ pseudo-huber ู ููุงุด 3 ูุฑฺฉุฒ ุฎูุดู ุจุฑุชุฑ
</div>

### ุชุงุจุน Correntropy

```python

loss_type = 'correntropy'
params = params_dict[loss_type]

z=(f"๐ Running clustering with {loss_type.upper()} loss...")
centers, labels = generalized_clustering(data, k, loss_type, params)



loss_func = get_loss_func(loss_type, params)
plot_representatives(loss_type, centers, labels, image_files, image_folder, loss_func,z)

```

<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/CORRENTROPY loss.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
ุญุงุตู ุฎูุดู ุจูุฏ ุจุง ุชุงุจุน ุถุฑุฑ Correntropy ู ููุงุด 3 ูุฑฺฉุฒ ุฎูุดู ุจุฑุชุฑ
</div>

### ุชุงุจุน Epsilon-Insensitive

```python

loss_type = 'epsilon_insensitive'
params = params_dict[loss_type]

z=(f"๐ Running clustering with {loss_type.upper()} loss...")
centers, labels = generalized_clustering(data, k, loss_type, params)



loss_func = get_loss_func(loss_type, params)
plot_representatives(loss_type, centers, labels, image_files, image_folder, loss_func,z)

```

<div style="display: flex; justify-content: center; align-items: center; gap: 10px;">
    <img src="/assets/patterneffort/ImageCluster/EPSILON_INSENSITIVE loss.png" alt="IPS1" style="width: 50%; height: 50%; object-fit: contain;">
</div>
<div class="caption" style="text-align: center; margin-top: 8px;">
ุญุงุตู ุฎูุดู ุจูุฏ ุจุง ุชุงุจุน ุถุฑุฑ huber ู ููุงุด 3 ูุฑฺฉุฒ ุฎูุดู ุจุฑุชุฑ
</div>

# ููุงุจุน 

- http://cnnlocalization.csail.mit.edu/
- https://docs.pytorch.org/vision/main/models.html
- https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html
- https://arxiv.org/abs/1409.1556